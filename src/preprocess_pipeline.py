import os
import logging
import time
import pandas as pd
import numpy as np
from dotenv import load_dotenv
from data_loader import DataLoader
from preprocessing.cleaner import DataCleaner
from preprocessing.transformer import DataTransformer
from preprocessing.feature_eng import FeatureEngineer
from mongodb_handler import MongoDBHandler
import argparse
import json

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler("preprocess.log"),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

class PreprocessingPipeline:
    """ë°ì´í„° ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸"""
    
    def __init__(self, data_dir=None, config=None):
        """
        ì´ˆê¸°í™” í•¨ìˆ˜
        
        Parameters:
            data_dir (str): ë°ì´í„° ë””ë ‰í† ë¦¬ ê²½ë¡œ
            config (dict): ì „ì²˜ë¦¬ ì„¤ì • (íŒŒì¼ë³„ ì „ì²˜ë¦¬ ë°©ë²• ì •ì˜)
        """
        self.data_dir = data_dir or os.getenv('DATA_DIR', './data')
        self.config = config or self._default_config()
        
        # êµ¬ì„± ìš”ì†Œ ì´ˆê¸°í™”
        self.data_loader = DataLoader(data_dir=self.data_dir)
        self.cleaner = DataCleaner()
        self.transformer = DataTransformer()
        self.feature_engineer = FeatureEngineer()
        self.mongodb_handler = MongoDBHandler()
        
        logger.info("ğŸš€ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” ì™„ë£Œ")
    
    def _default_config(self):
        """
        ê¸°ë³¸ ì „ì²˜ë¦¬ ì„¤ì • ë°˜í™˜
        
        Returns:
            dict: íŒŒì¼ë³„ ì „ì²˜ë¦¬ ì„¤ì •
        """
        return {
            # íŒŒì¼ ì´ë¦„ íŒ¨í„´ë³„ ì „ì²˜ë¦¬ ì„¤ì •
            "file_patterns": {
                "bid_data_*.csv": {
                    "log_transform_columns": ["ê¸°ì´ˆê¸ˆì•¡", "ì˜ˆì •ê¸ˆì•¡", "ì˜ˆê°€", "íˆ¬ì°°ê°€"],
                    "normalize_columns": ["ê¸°ì´ˆê¸ˆì•¡", "ì˜ˆì •ê¸ˆì•¡", "ì˜ˆê°€", "íˆ¬ì°°ê°€", "norm_log_ê¸°ì´ˆê¸ˆì•¡", "norm_log_ì˜ˆì •ê¸ˆì•¡"],
                    "categorical_columns": ["ê³µê³ ì¢…ë¥˜", "ì—…ì¢…", "ë‚™ì°°ë°©ë²•"],
                    "text_columns": ["ê³µê³ ì œëª©", "ê³µê³ ë‚´ìš©"],
                    "key_column": "ê³µê³ ë²ˆí˜¸",
                    "date_columns": ["ì…ì°°ì¼ì", "ê°œì°°ì¼ì‹œ"],
                    "target_columns": ["ê±°ë˜ì ì •ì„±", "ë‚™ì°°ê°€ê²©ë¹„ìœ¨"],
                    "required_columns": ["ê³µê³ ë²ˆí˜¸", "ê¸°ì´ˆê¸ˆì•¡", "ì˜ˆì •ê¸ˆì•¡"]
                },
                "notice_data_*.csv": {
                    "log_transform_columns": ["ê¸°ì´ˆê¸ˆì•¡", "ì˜ˆì •ê¸ˆì•¡"],
                    "normalize_columns": ["ê¸°ì´ˆê¸ˆì•¡", "ì˜ˆì •ê¸ˆì•¡", "norm_log_ê¸°ì´ˆê¸ˆì•¡", "norm_log_ì˜ˆì •ê¸ˆì•¡"],
                    "categorical_columns": ["ê³µê³ ì¢…ë¥˜", "ì—…ì¢…", "ë‚™ì°°ë°©ë²•"],
                    "text_columns": ["ê³µê³ ì œëª©", "ê³µê³ ë‚´ìš©"],
                    "key_column": "ê³µê³ ë²ˆí˜¸",
                    "date_columns": ["ì…ì°°ì¼ì", "ê°œì°°ì¼ì‹œ"],
                    "required_columns": ["ê³µê³ ë²ˆí˜¸", "ê¸°ì´ˆê¸ˆì•¡"]
                }
            },
            # ë°ì´í„°ì…‹ ìœ í˜•ë³„ ì„¤ì • (íŒŒì¼ëª… ê¸°ë°˜ìœ¼ë¡œ ì‹ë³„)
            "dataset_types": {
                "DataSet_3": {
                    "description": "3ê°œ ì…ì°°ê±´ ì´ìƒ ì°¸ì—¬ ì—…ì²´ ë°ì´í„°",
                    "priority": 1
                },
                "DataSet_2": {
                    "description": "2ê°œ ì…ì°°ê±´ ì°¸ì—¬ ì—…ì²´ ë°ì´í„°",
                    "priority": 2
                },
                "DataSet_etc": {
                    "description": "ê¸°íƒ€ ë°ì´í„°",
                    "priority": 3
                }
            },
            # ê³ ê¸‰ ì „ì²˜ë¦¬ ì„¤ì •
            "advanced_features": {
                "enabled": True,     # ê³ ê¸‰ ì „ì²˜ë¦¬ ì‚¬ìš© ì—¬ë¶€
                "pca_enabled": True,  # PCA ì°¨ì› ì¶•ì†Œ ì‚¬ìš© ì—¬ë¶€
                "embedding_enabled": True,  # í…ìŠ¤íŠ¸ ì„ë² ë”© ì‚¬ìš© ì—¬ë¶€
                "feature_selection_enabled": True  # íŠ¹ì„± ì„ íƒ ì‚¬ìš© ì—¬ë¶€
            }
        }
    
    def run(self, file_pattern=None, save_to_mongodb=True, custom_preprocessing=None, advanced_features=None):
        """
        ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
        
        Parameters:
            file_pattern (str): íŒŒì¼ íŒ¨í„´ (ì˜ˆ: '*.csv')
            save_to_mongodb (bool): MongoDBì— ì €ì¥ ì—¬ë¶€
            custom_preprocessing (callable): ì¶”ê°€ ì „ì²˜ë¦¬ í•¨ìˆ˜
            advanced_features (bool): ê³ ê¸‰ ì „ì²˜ë¦¬ ê¸°ëŠ¥ ì‚¬ìš© ì—¬ë¶€(ì„¤ì •ë³´ë‹¤ ìš°ì„ í•¨)
            
        Returns:
            dict: ì „ì²˜ë¦¬ëœ ë°ì´í„°ì…‹ ë”•ì…”ë„ˆë¦¬
        """
        start_time = time.time()
        logger.info("ğŸš€ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì‹œì‘")
        
        # ê³ ê¸‰ ì „ì²˜ë¦¬ ì„¤ì • ì´ˆê¸°í™”
        use_advanced = advanced_features if advanced_features is not None else self.config.get('advanced_features', {}).get('enabled', False)
        logger.info(f"ğŸ” ê³ ê¸‰ ì „ì²˜ë¦¬ ì‚¬ìš©: {'âœ… í™œì„±í™”' if use_advanced else 'âŒ ë¹„í™œì„±í™”'}")
        
        # 1. ë°ì´í„° ë¡œë“œ
        logger.info("ğŸ“‚ ë°ì´í„° ë¡œë“œ ë‹¨ê³„")
        dataset_dict = self.data_loader.load_raw_data(file_pattern)
        
        if not dataset_dict:
            logger.error("âŒ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: ë°ì´í„°ì…‹ì´ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")
            return {}
        
        # ë°ì´í„°ì…‹ ì •ë³´ ì¶œë ¥
        logger.info(f"âœ… {len(dataset_dict)}ê°œ ë°ì´í„°ì…‹ ë¡œë“œ ì™„ë£Œ:")
        for name, df in dataset_dict.items():
            logger.info(f"   - {name}: {df.shape[0]} í–‰ x {df.shape[1]} ì—´")
        
        # 2. ë°ì´í„° ì •ì œ
        logger.info("\nğŸ§¹ ë°ì´í„° ì •ì œ ë‹¨ê³„")
        cleaned_dict = {}
        for name, df in dataset_dict.items():
            # íŒŒì¼ ìœ í˜• ê¸°ë°˜ ì„¤ì • ì°¾ê¸°
            preprocessing_config = self._get_preprocessing_config(name)
            if preprocessing_config:
                logger.info(f"ğŸ” '{name}' ë°ì´í„°ì…‹ì— ë§ì¶¤í˜• ì „ì²˜ë¦¬ ì ìš©")
                # í•„ìˆ˜ ì»¬ëŸ¼ í™•ì¸
                if preprocessing_config.get("required_columns"):
                    self._validate_required_columns(df, preprocessing_config["required_columns"], name)
            
            # ê¸°ë³¸ ì •ì œ ì ìš©
            cleaned_dict[name] = self.cleaner.clean_dataset(df, name)
        
        # 3. ë°ì´í„° ë³€í™˜ (íŒŒì¼ ìœ í˜•ë³„ ë§ì¶¤ ì„¤ì • ì ìš©)
        logger.info("\nğŸ”„ ë°ì´í„° ë³€í™˜ ë‹¨ê³„")
        transformed_dict = {}
        for name, df in cleaned_dict.items():
            # íŒŒì¼ ìœ í˜• ê¸°ë°˜ ì„¤ì • ì°¾ê¸°
            preprocessing_config = self._get_preprocessing_config(name)
            
            # ë§ì¶¤í˜• ë¡œê·¸ ë³€í™˜ ë° ì •ê·œí™” ì ìš©
            if preprocessing_config:
                # ë¡œê·¸ ë³€í™˜ ì ìš©
                if preprocessing_config.get("log_transform_columns"):
                    df = self._apply_log_transform(df, preprocessing_config["log_transform_columns"], name)
                
                # ì •ê·œí™” ì ìš©
                if preprocessing_config.get("normalize_columns"):
                    # ê¸°ë³¸ ë³€í™˜ í”„ë¡œì„¸ìŠ¤ ì§„í–‰
                    df = self.transformer.transform_dataset(df, name)
                    
                    # ë‚ ì§œ ì»¬ëŸ¼ ì²˜ë¦¬
                    if preprocessing_config.get("date_columns"):
                        df = self._process_date_columns(df, preprocessing_config["date_columns"])
                else:
                    # ê¸°ë³¸ ë³€í™˜ í”„ë¡œì„¸ìŠ¤ ì§„í–‰
                    df = self.transformer.transform_dataset(df, name)
            else:
                # ê¸°ë³¸ ë³€í™˜ í”„ë¡œì„¸ìŠ¤ ì§„í–‰
                df = self.transformer.transform_dataset(df, name)
            
            transformed_dict[name] = df
        
        # 4. íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§ (íŒŒì¼ ì˜ì¡´ì )
        logger.info("\nğŸ”§ íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§ ë‹¨ê³„")
        engineered_dict = {}
        for name, df in transformed_dict.items():
            # íŒŒì¼ ìœ í˜• ê¸°ë°˜ ì„¤ì • ì°¾ê¸°
            preprocessing_config = self._get_preprocessing_config(name)
            
            # ë§ì¶¤í˜• íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§ ì ìš©
            if preprocessing_config:
                # í…ìŠ¤íŠ¸ íŠ¹ì„± ì¶”ì¶œ
                if preprocessing_config.get("text_columns"):
                    logger.info(f"ğŸ“ '{name}' ë°ì´í„°ì…‹ì˜ í…ìŠ¤íŠ¸ íŠ¹ì„± ì²˜ë¦¬ ì¤‘...")
                
                # ë²”ì£¼í˜• íŠ¹ì„± ì²˜ë¦¬
                if preprocessing_config.get("categorical_columns"):
                    logger.info(f"ğŸ·ï¸ '{name}' ë°ì´í„°ì…‹ì˜ ë²”ì£¼í˜• íŠ¹ì„± ì²˜ë¦¬ ì¤‘...")
                
                # íƒ€ê²Ÿ ì»¬ëŸ¼ ê³„ì‚°
                if preprocessing_config.get("target_columns"):
                    logger.info(f"ğŸ¯ '{name}' ë°ì´í„°ì…‹ì˜ íƒ€ê²Ÿ íŠ¹ì„± ê³„ì‚° ì¤‘...")
            
            # ê³ ê¸‰ íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§ ì ìš©
            df = self.feature_engineer.engineer_features(df, name, use_advanced)
            
            # ì‚¬ìš©ì ì •ì˜ ì „ì²˜ë¦¬ ì ìš© (ìˆëŠ” ê²½ìš°)
            if custom_preprocessing and callable(custom_preprocessing):
                logger.info(f"ğŸ› ï¸ '{name}' ë°ì´í„°ì…‹ì— ì‚¬ìš©ì ì •ì˜ ì „ì²˜ë¦¬ ì ìš© ì¤‘...")
                df = custom_preprocessing(df, name, preprocessing_config)
            
            engineered_dict[name] = df
        
        # 5. ì…ì°°ê°€ ë¶„ì„ íŠ¹í™” ì²˜ë¦¬ (í•„ìš”í•œ ê²½ìš°)
        logger.info("\nğŸ’° ì…ì°°ê°€ ë¶„ì„ íŠ¹í™” ì²˜ë¦¬ ë‹¨ê³„")
        for name, df in engineered_dict.items():
            preprocessing_config = self._get_preprocessing_config(name)
            if preprocessing_config and "ê¸°ì´ˆê¸ˆì•¡" in df.columns and "ì˜ˆì •ê¸ˆì•¡" in df.columns:
                # ê°€ê²©ë¹„ìœ¨ ê³„ì‚°
                if "íˆ¬ì°°ê°€" in df.columns:
                    df["ë‚™ì°°ê°€ê²©ë¹„ìœ¨"] = df["íˆ¬ì°°ê°€"] / df["ì˜ˆì •ê¸ˆì•¡"]
                    logger.info(f"âœ… '{name}' ë°ì´í„°ì…‹ì˜ ë‚™ì°°ê°€ê²©ë¹„ìœ¨ ê³„ì‚° ì™„ë£Œ")
                
                # ê¸°íƒ€ ì…ì°°ê°€ ê´€ë ¨ íŠ¹ì„± ìƒì„±
                self._create_bid_price_features(df, name)
            
            engineered_dict[name] = df
            
            # ë°ì´í„°ì…‹ í¬ê¸° ë° íŠ¹ì„± ì •ë³´ ì¶œë ¥
            logger.info(f"âœ… '{name}' ë°ì´í„°ì…‹ ì²˜ë¦¬ ì™„ë£Œ: {df.shape[0]}í–‰ x {df.shape[1]}ì—´")
            # ìƒìœ„ 20ê°œ ì»¬ëŸ¼ ì´ë¦„ ì¶œë ¥ (ì¤‘ìš” ì»¬ëŸ¼ í™•ì¸ìš©)
            col_sample = list(df.columns[:20])
            logger.info(f"   ìƒ˜í”Œ ì»¬ëŸ¼(20ê°œ): {col_sample}")
        
        # 6. MongoDBì— ì €ì¥ (ì„ íƒ ì‚¬í•­)
        if save_to_mongodb:
            logger.info("\nğŸ’¾ MongoDBì— ì €ì¥ ë‹¨ê³„")
            try:
                with self.mongodb_handler as mongo:
                    collection_names = mongo.save_datasets(engineered_dict)
                    logger.info(f"âœ… {len(collection_names)}ê°œ ë°ì´í„°ì…‹ì„ MongoDBì— ì €ì¥ ì™„ë£Œ")
                    for key, coll in collection_names.items():
                        logger.info(f"   - {key} -> {coll}")
            except Exception as e:
                logger.error(f"âŒ MongoDB ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        
        # ì²˜ë¦¬ ì‹œê°„ ê¸°ë¡
        elapsed_time = time.time() - start_time
        logger.info(f"\nâœ… ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì™„ë£Œ (ì†Œìš” ì‹œê°„: {elapsed_time:.2f}ì´ˆ)")
        
        return engineered_dict
    
    def _get_preprocessing_config(self, dataset_name):
        """
        ë°ì´í„°ì…‹ ì´ë¦„ì— ë”°ë¥¸ ì „ì²˜ë¦¬ ì„¤ì • ë°˜í™˜
        
        Parameters:
            dataset_name (str): ë°ì´í„°ì…‹ ì´ë¦„
            
        Returns:
            dict: ì „ì²˜ë¦¬ ì„¤ì •
        """
        # ë°ì´í„°ì…‹ ì´ë¦„ì—ì„œ íŒŒì¼ íŒ¨í„´ ì¶”ì¶œ
        import re
        
        # ê¸°ë³¸ ì„¤ì •
        default_config = None
        
        # íŒŒì¼ íŒ¨í„´ ë§¤ì¹­
        for pattern, config in self.config["file_patterns"].items():
            pattern_regex = pattern.replace("*", ".*").replace(".", "\.")
            if re.search(pattern_regex, dataset_name, re.IGNORECASE):
                return config
        
        # ë°ì´í„°ì…‹ ìœ í˜•ë³„ ì„¤ì •
        for dataset_type, type_config in self.config["dataset_types"].items():
            if dataset_type in dataset_name:
                # ë§Œì•½ íŒŒì¼ íŒ¨í„´ì´ ì—†ì§€ë§Œ ë°ì´í„°ì…‹ ìœ í˜•ì€ ì¼ì¹˜í•˜ëŠ” ê²½ìš°
                if default_config is None:
                    # ì²« ë²ˆì§¸ íŒŒì¼ íŒ¨í„´ ì„¤ì •ì„ ê¸°ë³¸ê°’ìœ¼ë¡œ ì‚¬ìš©
                    default_config = next(iter(self.config["file_patterns"].values()), {})
        
        return default_config
    
    def _validate_required_columns(self, df, required_columns, dataset_name):
        """
        í•„ìˆ˜ ì»¬ëŸ¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
        
        Parameters:
            df (DataFrame): ë°ì´í„°í”„ë ˆì„
            required_columns (list): í•„ìˆ˜ ì»¬ëŸ¼ ëª©ë¡
            dataset_name (str): ë°ì´í„°ì…‹ ì´ë¦„
            
        Raises:
            ValueError: í•„ìˆ˜ ì»¬ëŸ¼ì´ ì—†ëŠ” ê²½ìš°
        """
        missing_columns = [col for col in required_columns if col not in df.columns]
        if missing_columns:
            logger.error(f"âŒ '{dataset_name}' ë°ì´í„°ì…‹ì— í•„ìˆ˜ ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤: {missing_columns}")
            raise ValueError(f"í•„ìˆ˜ ì»¬ëŸ¼ ëˆ„ë½: {missing_columns}")
        
        logger.info(f"âœ… '{dataset_name}' ë°ì´í„°ì…‹ì˜ í•„ìˆ˜ ì»¬ëŸ¼ ê²€ì¦ ì™„ë£Œ")
    
    def _apply_log_transform(self, df, columns, dataset_name):
        """
        ì§€ì •ëœ ì»¬ëŸ¼ì— ë¡œê·¸ ë³€í™˜ ì ìš©
        
        Parameters:
            df (DataFrame): ë°ì´í„°í”„ë ˆì„
            columns (list): ë¡œê·¸ ë³€í™˜í•  ì»¬ëŸ¼ ëª©ë¡
            dataset_name (str): ë°ì´í„°ì…‹ ì´ë¦„
            
        Returns:
            DataFrame: ë¡œê·¸ ë³€í™˜ì´ ì ìš©ëœ ë°ì´í„°í”„ë ˆì„
        """
        for col in columns:
            if col in df.columns:
                try:
                    # ìŒìˆ˜ ê°’ ì²˜ë¦¬
                    if (df[col] <= 0).any():
                        min_val = abs(df[col].min()) + 1 if df[col].min() <= 0 else 0
                        logger.info(f"âš ï¸ '{col}' ì»¬ëŸ¼ì— 0 ì´í•˜ ê°’ì´ ìˆì–´ {min_val} ì¶”ê°€")
                        df[f"norm_log_{col}"] = np.log1p(df[col] + min_val)
                    else:
                        df[f"norm_log_{col}"] = np.log1p(df[col])
                    
                    logger.info(f"âœ… '{dataset_name}' ë°ì´í„°ì…‹ì˜ '{col}' ì»¬ëŸ¼ì— ë¡œê·¸ ë³€í™˜ ì ìš© -> 'norm_log_{col}'")
                except Exception as e:
                    logger.error(f"âŒ '{col}' ì»¬ëŸ¼ ë¡œê·¸ ë³€í™˜ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        
        return df
    
    def _process_date_columns(self, df, date_columns):
        """
        ë‚ ì§œ ì»¬ëŸ¼ ì²˜ë¦¬
        
        Parameters:
            df (DataFrame): ë°ì´í„°í”„ë ˆì„
            date_columns (list): ë‚ ì§œ ì»¬ëŸ¼ ëª©ë¡
            
        Returns:
            DataFrame: ë‚ ì§œ ì²˜ë¦¬ëœ ë°ì´í„°í”„ë ˆì„
        """
        for col in date_columns:
            if col in df.columns:
                try:
                    # ë‚ ì§œ í˜•ì‹ ë³€í™˜
                    df[col] = pd.to_datetime(df[col], errors='coerce')
                    
                    # ì—°ë„, ì›”, ì¼ ì¶”ì¶œ
                    df[f"{col}_year"] = df[col].dt.year
                    df[f"{col}_month"] = df[col].dt.month
                    df[f"{col}_day"] = df[col].dt.day
                    
                    # ìš”ì¼ ì¶”ì¶œ (0: ì›”ìš”ì¼, 6: ì¼ìš”ì¼)
                    df[f"{col}_dayofweek"] = df[col].dt.dayofweek
                    
                    # ë¶„ê¸° ì¶”ì¶œ
                    df[f"{col}_quarter"] = df[col].dt.quarter
                    
                    logger.info(f"âœ… '{col}' ì»¬ëŸ¼ ë‚ ì§œ íŠ¹ì„± ì¶”ì¶œ ì™„ë£Œ")
                except Exception as e:
                    logger.error(f"âŒ '{col}' ì»¬ëŸ¼ ë‚ ì§œ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        
        return df
    
    def _create_bid_price_features(self, df, dataset_name):
        """
        ì…ì°°ê°€ ê´€ë ¨ íŠ¹ì„± ìƒì„±
        
        Parameters:
            df (DataFrame): ë°ì´í„°í”„ë ˆì„
            dataset_name (str): ë°ì´í„°ì…‹ ì´ë¦„
            
        Returns:
            DataFrame: ì…ì°°ê°€ íŠ¹ì„±ì´ ì¶”ê°€ëœ ë°ì´í„°í”„ë ˆì„
        """
        try:
            # ê¸°ì´ˆê¸ˆì•¡ ëŒ€ë¹„ ì˜ˆì •ê¸ˆì•¡ ë¹„ìœ¨
            if "ê¸°ì´ˆê¸ˆì•¡" in df.columns and "ì˜ˆì •ê¸ˆì•¡" in df.columns:
                # 0ìœ¼ë¡œ ë‚˜ëˆ„ëŠ” ê²ƒ ë°©ì§€
                df["ì˜ˆì •ê°€ë¹„ìœ¨"] = df["ì˜ˆì •ê¸ˆì•¡"] / df["ê¸°ì´ˆê¸ˆì•¡"].replace(0, np.nan)
                df["ì˜ˆì •ê°€ë¹„ìœ¨"] = df["ì˜ˆì •ê°€ë¹„ìœ¨"].fillna(0)
                
                # ë¡œê·¸ ë³€í™˜ëœ ê°’ ì‚¬ì´ì˜ ë¹„ìœ¨ë„ ê³„ì‚°
                if "norm_log_ê¸°ì´ˆê¸ˆì•¡" in df.columns and "norm_log_ì˜ˆì •ê¸ˆì•¡" in df.columns:
                    df["log_ì˜ˆì •ê°€ë¹„ìœ¨"] = df["norm_log_ì˜ˆì •ê¸ˆì•¡"] / df["norm_log_ê¸°ì´ˆê¸ˆì•¡"].replace(0, np.nan)
                    df["log_ì˜ˆì •ê°€ë¹„ìœ¨"] = df["log_ì˜ˆì •ê°€ë¹„ìœ¨"].fillna(0)
            
            # ì¶”ê°€ ê°€ê²© ë¹„ìœ¨ íŠ¹ì„±ë“¤
            if "íˆ¬ì°°ê°€" in df.columns:
                if "ê¸°ì´ˆê¸ˆì•¡" in df.columns:
                    df["íˆ¬ì°°ê°€_ê¸°ì´ˆê¸ˆì•¡ë¹„ìœ¨"] = df["íˆ¬ì°°ê°€"] / df["ê¸°ì´ˆê¸ˆì•¡"].replace(0, np.nan)
                    df["íˆ¬ì°°ê°€_ê¸°ì´ˆê¸ˆì•¡ë¹„ìœ¨"] = df["íˆ¬ì°°ê°€_ê¸°ì´ˆê¸ˆì•¡ë¹„ìœ¨"].fillna(0)
                
                if "ì˜ˆì •ê¸ˆì•¡" in df.columns:
                    df["íˆ¬ì°°ê°€_ì˜ˆì •ê¸ˆì•¡ë¹„ìœ¨"] = df["íˆ¬ì°°ê°€"] / df["ì˜ˆì •ê¸ˆì•¡"].replace(0, np.nan)
                    df["íˆ¬ì°°ê°€_ì˜ˆì •ê¸ˆì•¡ë¹„ìœ¨"] = df["íˆ¬ì°°ê°€_ì˜ˆì •ê¸ˆì•¡ë¹„ìœ¨"].fillna(0)
            
            logger.info(f"âœ… '{dataset_name}' ë°ì´í„°ì…‹ì˜ ì…ì°°ê°€ ê´€ë ¨ íŠ¹ì„± ìƒì„± ì™„ë£Œ")
            
            return df
        except Exception as e:
            logger.error(f"âŒ ì…ì°°ê°€ ê´€ë ¨ íŠ¹ì„± ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return df

def get_args():
    """ëª…ë ¹ì¤„ ì¸ìˆ˜ íŒŒì‹±"""
    parser = argparse.ArgumentParser(description='ì…ì°°ê°€ ë¶„ì„ìš© ë°ì´í„° ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰')
    parser.add_argument('--file-pattern', type=str, default='*.csv',
                        help='ì²˜ë¦¬í•  íŒŒì¼ íŒ¨í„´ (ì˜ˆ: *.csv)')
    parser.add_argument('--no-mongo', action='store_true',
                        help='MongoDBì— ì €ì¥í•˜ì§€ ì•ŠìŒ')
    parser.add_argument('--advanced-features', action='store_true',
                        help='ê³ ê¸‰ ì „ì²˜ë¦¬ ê¸°ëŠ¥ ì‚¬ìš© (PCA, ì„ë² ë”©, íŠ¹ì„± ì„ íƒ ë“±)')
    return parser.parse_args()

if __name__ == "__main__":
    # ëª…ë ¹ì¤„ ì¸ìˆ˜ íŒŒì‹±
    args = get_args()
    
    # íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
    pipeline = PreprocessingPipeline()
    processed_data = pipeline.run(
        file_pattern=args.file_pattern,
        save_to_mongodb=not args.no_mongo,
        advanced_features=args.advanced_features
    )
    
    if processed_data:
        # ì²˜ë¦¬ëœ ë°ì´í„°ì…‹ ì •ë³´ ì¶œë ¥
        logger.info("\nğŸ“Š ì²˜ë¦¬ëœ ë°ì´í„°ì…‹ í†µê³„:")
        for name, df in processed_data.items():
            logger.info(f"\n{name}:")
            logger.info(f"  - í˜•íƒœ: {df.shape[0]} í–‰ x {df.shape[1]} ì—´")
            logger.info(f"  - ì»¬ëŸ¼: {', '.join(df.columns[:5])}... ì™¸ {max(0, len(df.columns)-5)}ê°œ")
            logger.info(f"  - ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {df.memory_usage(deep=True).sum() / (1024*1024):.2f} MB") 