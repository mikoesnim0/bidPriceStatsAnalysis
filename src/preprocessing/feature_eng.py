import pandas as pd
import numpy as np
import logging
from sklearn.decomposition import PCA
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.feature_selection import SelectKBest, f_regression
from sklearn.preprocessing import MinMaxScaler
import re
import nltk
from nltk.corpus import stopwords
import warnings
import torch
from transformers import AutoTokenizer, AutoModel
from tqdm import tqdm

# ê²½ê³  ë¬´ì‹œ
warnings.filterwarnings('ignore', category=UserWarning)

logger = logging.getLogger(__name__)

class FeatureEngineer:
    """íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§ì„ ìœ„í•œ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.models = {}  # íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§ ëª¨ë¸ ì €ì¥
        
        # NLTK ë¦¬ì†ŒìŠ¤ ë‹¤ìš´ë¡œë“œ (ì²« ì‚¬ìš©ì‹œ)
        try:
            nltk.data.find('corpora/stopwords')
        except LookupError:
            nltk.download('stopwords', quiet=True)
            nltk.download('punkt', quiet=True)
        
        # BGE-M3 ëª¨ë¸ ì´ˆê¸°í™” ë³€ìˆ˜
        self.bge_model = None
        self.bge_tokenizer = None
        self.device = None
    
    def engineer_features(self, df, dataset_name, advanced_features=True):
        """
        íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§ ìˆ˜í–‰
        
        Parameters:
            df (DataFrame): íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§í•  ë°ì´í„°í”„ë ˆì„
            dataset_name (str): ë°ì´í„°ì…‹ ì´ë¦„
            advanced_features (bool): ê³ ê¸‰ íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§ ì ìš© ì—¬ë¶€
            
        Returns:
            DataFrame: íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§ëœ ë°ì´í„°í”„ë ˆì„
        """
        logger.info(f"ğŸ”§ {dataset_name} ë°ì´í„°ì…‹ íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§ ì‹œì‘")
        
        # 1. í…ìŠ¤íŠ¸ í”¼ì²˜ ì¶”ì¶œ (ê³µê³  ì œëª© ë“±)
        df = self._extract_text_features(df, dataset_name)
        
        if advanced_features:
            # 2. BGE-M3 ì„ë² ë”© ì ìš© (í…ìŠ¤íŠ¸ ë°ì´í„°ê°€ ì¶©ë¶„í•œ ê²½ìš°)
            df = self._apply_bge_embeddings(df, dataset_name)
            
            # 3. ì°¨ì› ì¶•ì†Œ (PCA)
            df = self._apply_dimension_reduction(df, dataset_name)
            
            # 4. íŠ¹ì„± ì„ íƒ
            df = self._select_best_features(df, dataset_name)
        
        # 5. íŠ¹ì„± ì¡°í•© (ìƒˆë¡œìš´ íŠ¹ì„± ìƒì„±)
        df = self._create_combined_features(df)
        
        logger.info(f"âœ… {dataset_name} ë°ì´í„°ì…‹ íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§ ì™„ë£Œ")
        return df
    
    def _extract_text_features(self, df, dataset_name):
        """í…ìŠ¤íŠ¸ íŠ¹ì„± ì¶”ì¶œ (TF-IDF)"""
        # í…ìŠ¤íŠ¸ ì»¬ëŸ¼ ì°¾ê¸° (ì œëª©, ì„¤ëª… ë“±)
        text_columns = []
        for col in df.select_dtypes(include=['object']).columns:
            if any(keyword in col for keyword in ['ì œëª©', 'title', 'ë‚´ìš©', 'content', 'ì„¤ëª…', 'í•­ëª©', 'ê³µê³ ']):
                text_columns.append(col)
        
        if text_columns:
            for col in text_columns:
                # ê²°ì¸¡ì¹˜ ì²˜ë¦¬
                df[col] = df[col].fillna('')
                
                # ìµœì†Œ 30ê°œ ì´ìƒì˜ í–‰ì´ ìˆì„ ë•Œë§Œ TF-IDF ì ìš©
                if len(df) >= 30 and df[col].str.len().sum() > 100:
                    # ì „ì²˜ë¦¬ í•¨ìˆ˜
                    def preprocess_text(text):
                        if not isinstance(text, str):
                            return ""
                        # íŠ¹ìˆ˜ë¬¸ì ì œê±° ë° ì†Œë¬¸ì ë³€í™˜
                        text = re.sub(r'[^\w\s]', ' ', text)
                        return text.lower()
                    
                    # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
                    df[f"{col}_preprocessed"] = df[col].apply(preprocess_text)
                    
                    # TF-IDF ë²¡í„°í™”
                    tfidf = TfidfVectorizer(
                        max_features=10,  # ìƒìœ„ 10ê°œ íŠ¹ì„±ìœ¼ë¡œ ì¦ê°€
                        min_df=2,        # ìµœì†Œ 2ê°œ ë¬¸ì„œì— ë“±ì¥
                        ngram_range=(1, 2)  # ë‹¨ì–´, êµ¬(2ë‹¨ì–´) ëª¨ë‘ ì‚¬ìš©
                    )
                    
                    # í…ìŠ¤íŠ¸ ë²¡í„°í™”
                    try:
                        tfidf_matrix = tfidf.fit_transform(df[f"{col}_preprocessed"])
                        
                        # ë²¡í„°í™” ê²°ê³¼ë¥¼ ë°ì´í„°í”„ë ˆì„ì— ì¶”ê°€
                        feature_names = tfidf.get_feature_names_out()
                        tfidf_df = pd.DataFrame(
                            tfidf_matrix.toarray(),
                            columns=[f"TFIDF_{col}_{feature}" for feature in feature_names]
                        )
                        
                        # ê¸°ì¡´ ë°ì´í„°í”„ë ˆì„ê³¼ ë³‘í•©
                        df = pd.concat([df, tfidf_df], axis=1)
                        
                        # ëª¨ë¸ ì €ì¥
                        self.models[f"tfidf_{col}_{dataset_name}"] = tfidf
                        
                        # ì „ì²˜ë¦¬ëœ ì»¬ëŸ¼ ì œê±°
                        df = df.drop(columns=[f"{col}_preprocessed"])
                        
                        logger.info(f"âœ… '{col}' ì»¬ëŸ¼ì— TF-IDF ì ìš© (íŠ¹ì„± {len(feature_names)}ê°œ ì¶”ì¶œ)")
                    except Exception as e:
                        logger.warning(f"âš ï¸ '{col}' ì»¬ëŸ¼ TF-IDF ì ìš© ì‹¤íŒ¨: {e}")
                        # ì „ì²˜ë¦¬ëœ ì»¬ëŸ¼ ì œê±°
                        if f"{col}_preprocessed" in df.columns:
                            df = df.drop(columns=[f"{col}_preprocessed"])
        
        return df
    
    def _load_bge_model(self, model_name="BAAI/bge-m3"):
        """
        BGE-M3 ì„ë² ë”© ëª¨ë¸ ë¡œë“œ
        
        Parameters:
            model_name (str): ì‚¬ìš©í•  ëª¨ë¸ ì´ë¦„
            
        Returns:
            None (ë‚´ë¶€ ë³€ìˆ˜ì— ëª¨ë¸ ì„¤ì •)
        """
        # ëª¨ë¸ì´ ì´ë¯¸ ë¡œë“œë˜ì–´ ìˆëŠ” ê²½ìš° ë‹¤ì‹œ ë¡œë“œí•˜ì§€ ì•ŠìŒ
        if self.bge_model is not None and self.bge_tokenizer is not None:
            return
        
        logger.info(f"ğŸ”„ BGE-M3 ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì‹œì‘: {model_name}")
        
        try:
            # ë””ë°”ì´ìŠ¤ ì„¤ì • (GPU ì‚¬ìš© ê°€ëŠ¥ ì‹œ GPU, ì•„ë‹ˆë©´ CPU)
            self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
            
            # í† í¬ë‚˜ì´ì € ë° ëª¨ë¸ ë¡œë“œ
            self.bge_tokenizer = AutoTokenizer.from_pretrained(model_name)
            self.bge_model = AutoModel.from_pretrained(model_name)
            
            # ëª¨ë¸ì„ ì§€ì •ëœ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
            self.bge_model.to(self.device)
            self.bge_model.eval()  # í‰ê°€ ëª¨ë“œë¡œ ì„¤ì •
            
            logger.info(f"âœ… BGE-M3 ì„ë² ë”© ëª¨ë¸ '{model_name}'ì„(ë¥¼) {self.device}ì— ì„±ê³µì ìœ¼ë¡œ ë¡œë“œí•¨")
            
        except Exception as e:
            logger.error(f"âŒ BGE-M3 ëª¨ë¸ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            self.bge_model = None
            self.bge_tokenizer = None
            self.device = None
            raise RuntimeError(f"âŒ BGE-M3 ëª¨ë¸ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    
    def _get_bge_embedding(self, texts, batch_size=32, max_length=512):
        """
        BGE-M3 ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ì˜ ì„ë² ë”© ë²¡í„° ìƒì„±
        
        Parameters:
            texts (list): ì„ë² ë”©í•  í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
            batch_size (int): ë°°ì¹˜ í¬ê¸°
            max_length (int): ìµœëŒ€ í† í° ê¸¸ì´
            
        Returns:
            list: ì„ë² ë”© ë²¡í„° ë¦¬ìŠ¤íŠ¸
        """
        if not texts:
            return []
        
        # ê²°ì¸¡ê°’ì„ ë¹ˆ ë¬¸ìì—´ë¡œ ë³€í™˜
        texts = [str(t) if pd.notnull(t) else "" for t in texts]
        
        embeddings = []
        num_batches = (len(texts) + batch_size - 1) // batch_size
        
        # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì²˜ë¦¬
        for i in range(0, len(texts), batch_size):
            batch_texts = texts[i:i+batch_size]
            
            # í…ìŠ¤íŠ¸ í† í°í™” ë° ëª¨ë¸ ì…ë ¥ ë°ì´í„° ì¤€ë¹„
            inputs = self.bge_tokenizer(
                batch_texts, 
                return_tensors="pt", 
                truncation=True, 
                padding=True, 
                max_length=max_length
            )
            
            # ì…ë ¥ì„ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
            inputs = {key: value.to(self.device) for key, value in inputs.items()}
            
            # ì¶”ë¡  ì‹¤í–‰ (ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚° ì—†ì´)
            with torch.no_grad():
                outputs = self.bge_model(**inputs)
            
            # CLS í† í° ì„ë² ë”© ì¶”ì¶œ ë° CPUë¡œ ì´ë™
            batch_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()
            embeddings.extend(batch_embeddings)
        
        return embeddings
    
    def _apply_bge_embeddings(self, df, dataset_name):
        """BGE-M3 ì„ë² ë”© ëª¨ë¸ì„ ì ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„±"""
        # í…ìŠ¤íŠ¸ ì»¬ëŸ¼ ì°¾ê¸°
        text_columns = []
        for col in df.select_dtypes(include=['object']).columns:
            if any(keyword in col for keyword in ['ì œëª©', 'title', 'ë‚´ìš©', 'content', 'ì„¤ëª…', 'í•­ëª©', 'ê³µê³ ']):
                text_columns.append(col)
        
        if not text_columns or len(df) < 30:  # ìµœì†Œ 30ê°œ ì´ìƒì˜ í–‰ì´ ìˆì„ ë•Œë§Œ ì ìš©
            return df
        
        try:
            # BGE-M3 ëª¨ë¸ ë¡œë“œ
            self._load_bge_model()
            
            if self.bge_model is None or self.bge_tokenizer is None:
                logger.warning("âš ï¸ BGE-M3 ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•„ ì„ë² ë”©ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
                return df
            
            # ê° í…ìŠ¤íŠ¸ ì»¬ëŸ¼ì— ëŒ€í•´ ì„ë² ë”© ì ìš©
            for col in text_columns:
                if df[col].str.len().sum() < 200:  # í…ìŠ¤íŠ¸ ì–‘ì´ ë„ˆë¬´ ì ìœ¼ë©´ ê±´ë„ˆëœ€
                    continue
                
                logger.info(f"ğŸ”„ '{col}' ì»¬ëŸ¼ì— BGE-M3 ì„ë² ë”© ì ìš© ì¤‘...")
                
                # í…ìŠ¤íŠ¸ ë°ì´í„° ì¶”ì¶œ ë° ê²°ì¸¡ì¹˜ ì²˜ë¦¬
                texts = df[col].fillna("").tolist()
                
                # ì„ë² ë”© ìƒì„±
                embeddings = self._get_bge_embedding(texts)
                
                if not embeddings:
                    logger.warning(f"âš ï¸ '{col}' ì»¬ëŸ¼ ì„ë² ë”© ê²°ê³¼ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")
                    continue
                
                # ì„ë² ë”© ê²°ê³¼ë¥¼ ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë³€í™˜
                embedding_df = pd.DataFrame(
                    embeddings,
                    columns=[f"BGE_{col}_{i}" for i in range(embeddings[0].shape[0])]
                )
                
                # ì„ë² ë”© ì°¨ì›ì´ ë„ˆë¬´ í¬ë©´ ì²˜ìŒ 10ê°œ ì°¨ì›ë§Œ ì‚¬ìš©
                if embedding_df.shape[1] > 10:
                    logger.info(f"âœ‚ï¸ '{col}' ì„ë² ë”© ì°¨ì›ì„ {embedding_df.shape[1]}ì—ì„œ 10ìœ¼ë¡œ ì¶•ì†Œí•©ë‹ˆë‹¤.")
                    embedding_df = embedding_df.iloc[:, :10]
                    embedding_df.columns = [f"BGE_{col}_{i}" for i in range(10)]
                
                # ê¸°ì¡´ ë°ì´í„°í”„ë ˆì„ê³¼ ë³‘í•©
                df = pd.concat([df, embedding_df.reset_index(drop=True)], axis=1)
                
                logger.info(f"âœ… '{col}' ì»¬ëŸ¼ì— BGE-M3 ì„ë² ë”© ì ìš© ì™„ë£Œ ({embedding_df.shape[1]}ì°¨ì›)")
                
        except Exception as e:
            logger.warning(f"âš ï¸ BGE-M3 ì„ë² ë”© ì ìš© ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        
        return df
    
    def _apply_dimension_reduction(self, df, dataset_name):
        """PCAë¥¼ ì´ìš©í•œ ì°¨ì› ì¶•ì†Œ"""
        # ì—°ì†í˜• ìˆ˜ì¹˜ ë³€ìˆ˜ë§Œ ì„ íƒ
        numeric_cols = df.select_dtypes(include=['float64', 'int64']).columns.tolist()
        
        # TFIDFë‚˜ ì›-í•« ì¸ì½”ë”© ê²°ê³¼ëŠ” ì œì™¸ (ì´ë¯¸ íŠ¹ì„±ìœ¼ë¡œ ì‚¬ìš©)
        numeric_cols = [col for col in numeric_cols if not any(prefix in col for prefix in ['TFIDF_', 'dummy_', 'BGE_', 'PCA_'])]
        
        if len(numeric_cols) >= 5:  # ìµœì†Œ 5ê°œ ì´ìƒì˜ ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ì´ ìˆì„ ë•Œë§Œ PCA ì ìš©
            # ê²°ì¸¡ì¹˜ ì±„ìš°ê¸°
            df_numeric = df[numeric_cols].fillna(0)
            
            try:
                # ë°ì´í„° ì •ê·œí™” (PCA ì „)
                scaler = MinMaxScaler()
                scaled_data = scaler.fit_transform(df_numeric)
                
                # PCA ì»´í¬ë„ŒíŠ¸ ìˆ˜ ê²°ì • (íŠ¹ì„± ìˆ˜ì— ë”°ë¼ ë™ì ìœ¼ë¡œ)
                n_components = min(3, len(numeric_cols) - 1)
                
                # PCA ì ìš©
                pca = PCA(n_components=n_components)
                pca_result = pca.fit_transform(scaled_data)
                
                # PCA ê²°ê³¼ë¥¼ ë°ì´í„°í”„ë ˆì„ì— ì¶”ê°€
                pca_df = pd.DataFrame(
                    pca_result,
                    columns=[f"PCA_{i+1}" for i in range(pca_result.shape[1])]
                )
                
                # ê¸°ì¡´ ë°ì´í„°í”„ë ˆì„ê³¼ ë³‘í•©
                df = pd.concat([df, pca_df], axis=1)
                
                # ëª¨ë¸ ì €ì¥
                self.models[f"pca_{dataset_name}"] = pca
                self.models[f"scaler_{dataset_name}"] = scaler
                
                # ì„¤ëª…ëœ ë¶„ì‚° ë¹„ìœ¨ ê¸°ë¡
                variance_ratio = pca.explained_variance_ratio_
                logger.info(f"âœ… PCA ì ìš© ì™„ë£Œ (ì„¤ëª…ëœ ë¶„ì‚° ë¹„ìœ¨: {[f'{v:.2%}' for v in variance_ratio]})")
                
            except Exception as e:
                logger.warning(f"âš ï¸ PCA ì ìš© ì‹¤íŒ¨: {e}")
        
        return df
    
    def _select_best_features(self, df, dataset_name):
        """íŠ¹ì„± ì„ íƒ (íƒ€ê²Ÿ ë³€ìˆ˜ê°€ ìˆëŠ” ê²½ìš°)"""
        target_vars = [col for col in df.columns if col in ['ê±°ë˜ì ì •ì„±', 'ë‚™ì°°ê°€ê²©ë¹„ìœ¨', 'íˆ¬ì°°ë¥ ']]
        
        if not target_vars:
            return df  # íƒ€ê²Ÿ ë³€ìˆ˜ê°€ ì—†ìœ¼ë©´ ì›ë˜ ë°ì´í„°í”„ë ˆì„ ë°˜í™˜
        
        # ì„ íƒí•  íƒ€ê²Ÿ ë³€ìˆ˜
        target_var = target_vars[0]
        
        # ì˜ˆì¸¡ ë³€ìˆ˜ ì„ íƒ (ìˆ«ìí˜• ì»¬ëŸ¼)
        num_cols = df.select_dtypes(include=['float64', 'int64']).columns.tolist()
        feature_cols = [col for col in num_cols if col != target_var and not col.startswith('selected_')]
        
        # ìµœì†Œ 5ê°œ ì´ìƒì˜ íŠ¹ì„±ê³¼ 30ê°œ ì´ìƒì˜ í–‰ì´ ìˆëŠ” ê²½ìš°ì—ë§Œ ì ìš©
        if len(feature_cols) >= 5 and len(df) >= 30:
            # ê²°ì¸¡ì¹˜ ì±„ìš°ê¸°
            X = df[feature_cols].fillna(0)
            y = df[target_var].fillna(0)
            
            try:
                # ìƒìœ„ kê°œ íŠ¹ì„± ì„ íƒ (f_regression ì‚¬ìš©)
                k = min(10, len(feature_cols))  # ìµœëŒ€ 10ê°œ íŠ¹ì„± ì„ íƒ
                selector = SelectKBest(f_regression, k=k)
                X_new = selector.fit_transform(X, y)
                
                # ì„ íƒëœ íŠ¹ì„± ì¸ë±ìŠ¤
                selected_indices = selector.get_support(indices=True)
                selected_features = [feature_cols[i] for i in selected_indices]
                
                # ì„ íƒëœ íŠ¹ì„±ì„ ìƒˆ ì—´ë¡œ ì¶”ê°€
                X_selected = pd.DataFrame(X_new, columns=[f"selected_{feature}" for feature in selected_features])
                
                # ê¸°ì¡´ ë°ì´í„°í”„ë ˆì„ê³¼ ë³‘í•©
                df = pd.concat([df, X_selected], axis=1)
                
                # ëª¨ë¸ ì €ì¥
                self.models[f"feature_selector_{dataset_name}"] = selector
                
                logger.info(f"âœ… íŠ¹ì„± ì„ íƒ ì™„ë£Œ (ìƒìœ„ {k}ê°œ íŠ¹ì„± ì„ íƒë¨: {', '.join(selected_features[:3])}... ì™¸ {len(selected_features)-3}ê°œ)")
            except Exception as e:
                logger.warning(f"âš ï¸ íŠ¹ì„± ì„ íƒ ì ìš© ì‹¤íŒ¨: {e}")
        
        return df
    
    def _create_combined_features(self, df):
        """ìƒˆë¡œìš´ íŠ¹ì„± ì¡°í•© ìƒì„±"""
        # 1. ê¸ˆì•¡ ê´€ë ¨ íŠ¹ì„± ì¡°í•©
        amount_cols = []
        for col in df.columns:
            if 'norm_log_' in col and any(keyword in col for keyword in ['ê¸ˆì•¡', 'price', 'amount']):
                amount_cols.append(col)
        
        # 2ê°œ ì´ìƒì˜ ê¸ˆì•¡ ì»¬ëŸ¼ì´ ìˆëŠ” ê²½ìš°
        if len(amount_cols) >= 2:
            for i in range(len(amount_cols)):
                for j in range(i+1, len(amount_cols)):
                    col1 = amount_cols[i]
                    col2 = amount_cols[j]
                    
                    # ë¹„ìœ¨ íŠ¹ì„± ìƒì„±
                    ratio_name = f"ratio_{col1.split('_', 2)[-1]}_{col2.split('_', 2)[-1]}"
                    df[ratio_name] = df[col1] / df[col2].replace(0, np.nan)
                    df[ratio_name] = df[ratio_name].fillna(0)
                    
                    # ì°¨ì´ íŠ¹ì„± ìƒì„±
                    diff_name = f"diff_{col1.split('_', 2)[-1]}_{col2.split('_', 2)[-1]}"
                    df[diff_name] = df[col1] - df[col2]
                    
                    logger.info(f"âœ… '{col1}'ì™€ '{col2}'ì˜ ë¹„ìœ¨ ë° ì°¨ì´ íŠ¹ì„± ìƒì„±")
        
        # 2. ë‚ ì§œ ê°„ ì°¨ì´ íŠ¹ì„± ìƒì„±
        date_cols = [col for col in df.columns if 'date' in col.lower() or 'ì¼ì' in col]
        
        if len(date_cols) >= 2:
            for i in range(len(date_cols)):
                for j in range(i+1, len(date_cols)):
                    try:
                        # ë‚ ì§œ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
                        df[date_cols[i]] = pd.to_datetime(df[date_cols[i]], errors='coerce')
                        df[date_cols[j]] = pd.to_datetime(df[date_cols[j]], errors='coerce')
                        
                        # ì¼ìˆ˜ ì°¨ì´ ê³„ì‚°
                        days_diff = f"days_between_{date_cols[i]}_{date_cols[j]}"
                        df[days_diff] = (df[date_cols[i]] - df[date_cols[j]]).dt.days
                        
                        logger.info(f"âœ… '{date_cols[i]}'ì™€ '{date_cols[j]}' ì‚¬ì´ì˜ ì¼ìˆ˜ ì°¨ì´ íŠ¹ì„± ìƒì„±")
                    except Exception as e:
                        logger.warning(f"âš ï¸ ë‚ ì§œ ì°¨ì´ íŠ¹ì„± ìƒì„± ì‹¤íŒ¨: {e}")
        
        return df 