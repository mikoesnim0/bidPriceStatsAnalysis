#!/usr/bin/env python
"""
ì…ì°°ê°€ ë°ì´í„° ì „ì²˜ë¦¬ ë° MongoDB ì—…ë¡œë“œ ìŠ¤í¬ë¦½íŠ¸
"""
import os
import argparse
import time
import logging
import pandas as pd
from tqdm import tqdm
from dotenv import load_dotenv
from preprocess_pipeline import PreprocessingPipeline
from mongodb_handler import MongoDBHandler
from pipeline_visualizer import create_visualizer

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler("preprocess_upload.log"),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

def main():
    # ëª…ë ¹í–‰ ì¸ì íŒŒì‹±
    parser = argparse.ArgumentParser(description='ì…ì°°ê°€ ë°ì´í„° ì „ì²˜ë¦¬ ë° MongoDB ì—…ë¡œë“œ')
    parser.add_argument('--data-dir', type=str, default=None, 
                        help='ë°ì´í„° ë””ë ‰í† ë¦¬ ê²½ë¡œ (ê¸°ë³¸ê°’: í™˜ê²½ ë³€ìˆ˜ ë˜ëŠ” ./data)')
    parser.add_argument('--file-pattern', type=str, default='*.csv',
                        help='ì²˜ë¦¬í•  íŒŒì¼ íŒ¨í„´ (ê¸°ë³¸ê°’: *.csv)')
    parser.add_argument('--output-dir', type=str, default='results',
                        help='ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬ (ê¸°ë³¸ê°’: results)')
    parser.add_argument('--skip-upload', action='store_true',
                        help='MongoDB ì—…ë¡œë“œ ê±´ë„ˆë›°ê¸°')
    parser.add_argument('--check-only', action='store_true',
                        help='MongoDB ë°ì´í„° í™•ì¸ë§Œ ìˆ˜í–‰')
    parser.add_argument('--clear-db', action='store_true',
                        help='ê¸°ì¡´ MongoDB ì»¬ë ‰ì…˜ ì œê±° í›„ ìƒˆë¡œ ì €ì¥')
    parser.add_argument('--generate-report', action='store_true',
                        help='íŒŒì´í”„ë¼ì¸ ë³´ê³ ì„œ ìƒì„±')
    parser.add_argument('--show-visualization', action='store_true',
                        help='íŒŒì´í”„ë¼ì¸ ì‹œê°í™” í‘œì‹œ')
    parser.add_argument('--advanced-features', action='store_true',
                        help='ê³ ê¸‰ ì „ì²˜ë¦¬ ê¸°ëŠ¥ ì‚¬ìš© (PCA, ì›Œë“œ ì„ë² ë”©, íŠ¹ì„± ì„ íƒ ë“±)')
    parser.add_argument('--use-bge-m3', action='store_true',
                        help='BGE-M3 ì„ë² ë”© ëª¨ë¸ ì‚¬ìš© (ê³ í’ˆì§ˆ ì„ë² ë”© ìƒì„±)')
    parser.add_argument('--bge-model-name', type=str, default='BAAI/bge-m3',
                        help='ì‚¬ìš©í•  BGE ëª¨ë¸ ì´ë¦„ (ê¸°ë³¸ê°’: BAAI/bge-m3)')
    args = parser.parse_args()
    
    # ë””ë ‰í† ë¦¬ ìƒì„±
    os.makedirs(args.output_dir, exist_ok=True)
    
    # íŒŒì´í”„ë¼ì¸ ì‹œê°í™” ê°ì²´ ìƒì„±
    visualizer = create_visualizer("ì…ì°°ê°€ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸", args.output_dir)
    visualizer.start_pipeline()
    
    # MongoDB ë°ì´í„° í™•ì¸ë§Œ ìˆ˜í–‰í•˜ëŠ” ê²½ìš°
    if args.check_only:
        step_idx = visualizer.add_step("MongoDB ë°ì´í„° í™•ì¸", "MongoDBì— ì €ì¥ëœ ë°ì´í„° í™•ì¸")
        check_result = check_mongodb_data(visualizer, step_idx)
        visualizer.complete_step(step_idx, {"ê²€ì‚¬í•œ ì»¬ë ‰ì…˜ ìˆ˜": check_result["collections_checked"]})
        visualizer.end_pipeline()
        
        if args.generate_report:
            report_file = visualizer.generate_report()
            print(f"\nğŸ“‹ íŒŒì´í”„ë¼ì¸ ë³´ê³ ì„œê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤: {report_file}")
        
        return
    
    # ê¸°ì¡´ MongoDB ì»¬ë ‰ì…˜ ì‚­ì œ
    if args.clear_db:
        step_idx = visualizer.add_step("MongoDB ì»¬ë ‰ì…˜ ì´ˆê¸°í™”", "ê¸°ì¡´ MongoDB ì»¬ë ‰ì…˜ ì‚­ì œ")
        clear_result = clear_mongodb_collections(visualizer, step_idx)
        visualizer.complete_step(step_idx, {"ì‚­ì œëœ ì»¬ë ‰ì…˜ ìˆ˜": clear_result["collections_dropped"]})
    
    # ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
    if not args.check_only:
        try:
            # ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ë‹¨ê³„ ì¶”ê°€
            data_load_step = visualizer.add_step("ë°ì´í„° ë¡œë“œ", "ì›ì‹œ ë°ì´í„° íŒŒì¼ ë¡œë“œ")
            
            # ê³ ê¸‰ ì „ì²˜ë¦¬ ì„¤ì • ë¡œê¹…
            advanced_enabled = args.advanced_features
            logger.info(f"ğŸ” ê³ ê¸‰ ì „ì²˜ë¦¬ ê¸°ëŠ¥: {'âœ… í™œì„±í™”ë¨' if advanced_enabled else 'âŒ ë¹„í™œì„±í™”ë¨'}")
            if advanced_enabled:
                logger.info("ê³ ê¸‰ ê¸°ëŠ¥ì—ëŠ” PCA, ì›Œë“œ ì„ë² ë”©, íŠ¹ì„± ì„ íƒ ë“±ì´ í¬í•¨ë©ë‹ˆë‹¤")
            
            # BGE-M3 ì„ë² ë”© ì„¤ì • ë¡œê¹…
            bge_enabled = args.use_bge_m3
            if bge_enabled:
                logger.info(f"ğŸ” BGE-M3 ì„ë² ë”© ëª¨ë¸: âœ… í™œì„±í™”ë¨ (ëª¨ë¸: {args.bge_model_name})")
            
            # ì‚¬ìš©ì ì •ì˜ ì „ì²˜ë¦¬ í•¨ìˆ˜ ì •ì˜
            def custom_preprocessing(df, name, config):
                """
                ì‚¬ìš©ì ì •ì˜ ì „ì²˜ë¦¬ í•¨ìˆ˜
                
                Parameters:
                    df (DataFrame): ì „ì²˜ë¦¬í•  ë°ì´í„°í”„ë ˆì„
                    name (str): ë°ì´í„°ì…‹ ì´ë¦„
                    config (dict): ì „ì²˜ë¦¬ ì„¤ì •
    
    Returns:
                    DataFrame: ì „ì²˜ë¦¬ëœ ë°ì´í„°í”„ë ˆì„
                """
                # BGE-M3 ì„ë² ë”© ëª¨ë¸ ì‚¬ìš©ì´ í™œì„±í™”ëœ ê²½ìš°
                # í•„ìš”í•œ ì²˜ë¦¬ëŠ” ì´ë¯¸ FeatureEngineerì—ì„œ ìˆ˜í–‰ë˜ë¯€ë¡œ ì´ í•¨ìˆ˜ëŠ” ìŠ¤í‚µë¨
                return df
            
            # ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”
            pipeline = PreprocessingPipeline(data_dir=args.data_dir)
            
            # íŒŒì´í”„ë¼ì¸ ì„¤ì • ì—…ë°ì´íŠ¸
            pipeline.config["advanced_features"]["enabled"] = advanced_enabled
            if bge_enabled:
                pipeline.config["bge_model"] = {
                    "enabled": True,
                    "model_name": args.bge_model_name
                }
            
            # íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ (ê° ë‹¨ê³„ë³„ë¡œ ì§„í–‰)
            processed_data = {}
            
            # 1. ë°ì´í„° ë¡œë“œ
            try:
                # ë°ì´í„° ë¡œë“œ
                dataset_dict = pipeline.data_loader.load_raw_data(args.file_pattern)
                
                if not dataset_dict:
                    visualizer.complete_step(data_load_step, {"ë¡œë“œëœ ë°ì´í„°ì…‹": 0})
                    raise ValueError("ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: ë°ì´í„°ì…‹ì´ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")
                
                # ë°ì´í„° ë¡œë“œ ë‹¨ê³„ ì™„ë£Œ
                visualizer.complete_step(data_load_step, {
                    "ë¡œë“œëœ ë°ì´í„°ì…‹": len(dataset_dict),
                    "ì´ ë ˆì½”ë“œ ìˆ˜": sum(len(df) for df in dataset_dict.values())
                })
                
                # 2. ë°ì´í„° ì •ì œ
                cleaning_step = visualizer.add_step("ë°ì´í„° ì •ì œ", "ê²°ì¸¡ì¹˜ ì²˜ë¦¬, ì¤‘ë³µ ì œê±°, ë°ì´í„° íƒ€ì… ë³€í™˜")
                cleaned_dict = {}
                
                pbar = visualizer.create_progress_bar(cleaning_step, len(dataset_dict), "ğŸ§¹ ë°ì´í„° ì •ì œ ì§„í–‰")
                for name, df in dataset_dict.items():
                    # íŒŒì¼ ìœ í˜• ê¸°ë°˜ ì„¤ì • ì°¾ê¸°
                    preprocessing_config = pipeline._get_preprocessing_config(name)
                    if preprocessing_config:
                        logger.info(f"ğŸ” '{name}' ë°ì´í„°ì…‹ì— ë§ì¶¤í˜• ì „ì²˜ë¦¬ ì ìš©")
                        # í•„ìˆ˜ ì»¬ëŸ¼ í™•ì¸
                        if preprocessing_config.get("required_columns"):
                            pipeline._validate_required_columns(df, preprocessing_config["required_columns"], name)
                    
                    # ê¸°ë³¸ ì •ì œ ì ìš©
                    cleaned_dict[name] = pipeline.cleaner.clean_dataset(df, name)
                    pbar.update(1)
                
                pbar.close()
                
                # ë°ì´í„°ì…‹ ì •ì œ ë‹¨ê³„ ì™„ë£Œ
                cleaning_metrics = {
                    "ì²˜ë¦¬ëœ ë°ì´í„°ì…‹": len(cleaned_dict),
                    "ì œê±°ëœ ê²°ì¸¡ì¹˜": sum(pipeline.cleaner.cleaning_stats[name]['missing_values_removed'] for name in cleaned_dict),
                    "ì œê±°ëœ ì¤‘ë³µ": sum(pipeline.cleaner.cleaning_stats[name]['duplicates_removed'] for name in cleaned_dict)
                }
                visualizer.complete_step(cleaning_step, cleaning_metrics)
                
                # 3. ë°ì´í„° ë³€í™˜
                transform_step = visualizer.add_step("ë°ì´í„° ë³€í™˜", "ë¡œê·¸ ë³€í™˜, ì •ê·œí™”, ì¸ì½”ë”©")
                transformed_dict = {}
                
                pbar = visualizer.create_progress_bar(transform_step, len(cleaned_dict), "ğŸ”„ ë°ì´í„° ë³€í™˜ ì§„í–‰")
                for name, df in cleaned_dict.items():
                    # íŒŒì¼ ìœ í˜• ê¸°ë°˜ ì„¤ì • ì°¾ê¸°
                    preprocessing_config = pipeline._get_preprocessing_config(name)
                    
                    # ë§ì¶¤í˜• ë¡œê·¸ ë³€í™˜ ë° ì •ê·œí™” ì ìš©
                    if preprocessing_config:
                        # ë¡œê·¸ ë³€í™˜ ì ìš©
                        if preprocessing_config.get("log_transform_columns"):
                            df = pipeline._apply_log_transform(df, preprocessing_config["log_transform_columns"], name)
                        
                        # ì •ê·œí™” ì ìš©
                        if preprocessing_config.get("normalize_columns"):
                            # ê¸°ë³¸ ë³€í™˜ í”„ë¡œì„¸ìŠ¤ ì§„í–‰
                            df = pipeline.transformer.transform_dataset(df, name)
                            
                            # ë‚ ì§œ ì»¬ëŸ¼ ì²˜ë¦¬
                            if preprocessing_config.get("date_columns"):
                                df = pipeline._process_date_columns(df, preprocessing_config["date_columns"])
                        else:
                            # ê¸°ë³¸ ë³€í™˜ í”„ë¡œì„¸ìŠ¤ ì§„í–‰
                            df = pipeline.transformer.transform_dataset(df, name)
                    else:
                        # ê¸°ë³¸ ë³€í™˜ í”„ë¡œì„¸ìŠ¤ ì§„í–‰
                        df = pipeline.transformer.transform_dataset(df, name)
                    
                    transformed_dict[name] = df
                    pbar.update(1)
                
                pbar.close()
                
                # ë°ì´í„° ë³€í™˜ ë‹¨ê³„ ì™„ë£Œ
                transform_metrics = {
                    "ì²˜ë¦¬ëœ ë°ì´í„°ì…‹": len(transformed_dict),
                    "ìƒì„±ëœ íŠ¹ì„± ìˆ˜": sum(len(df.columns) - len(cleaned_dict[name].columns) for name, df in transformed_dict.items())
                }
                visualizer.complete_step(transform_step, transform_metrics)
                
                # 4. íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§
                feature_step = visualizer.add_step("íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§", "í…ìŠ¤íŠ¸ ì²˜ë¦¬, ì°¨ì› ì¶•ì†Œ, íŠ¹ì„± ì¡°í•©")
                engineered_dict = {}
                
                # ê³ ê¸‰ íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§ í™œì„±í™” ì‹œ ë‹¨ê³„ ì œëª© ì—…ë°ì´íŠ¸
                if advanced_enabled:
                    feature_step_title = "ê³ ê¸‰ íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§"
                    feature_step_desc = "í…ìŠ¤íŠ¸ ì„ë² ë”©, PCA, íŠ¹ì„± ì„ íƒ, íŠ¹ì„± ì¡°í•©"
                    visualizer.update_step(feature_step, feature_step_title, feature_step_desc)
                
                pbar = visualizer.create_progress_bar(feature_step, len(transformed_dict), "ğŸ”§ íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§ ì§„í–‰")
                for name, df in transformed_dict.items():
                    # íŒŒì¼ ìœ í˜• ê¸°ë°˜ ì„¤ì • ì°¾ê¸°
                    preprocessing_config = pipeline._get_preprocessing_config(name)
                    
                    # ë§ì¶¤í˜• íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§ ì ìš©
                    if preprocessing_config:
                        # í…ìŠ¤íŠ¸ íŠ¹ì„± ì¶”ì¶œ
                        if preprocessing_config.get("text_columns"):
                            logger.info(f"ğŸ“ '{name}' ë°ì´í„°ì…‹ì˜ í…ìŠ¤íŠ¸ íŠ¹ì„± ì²˜ë¦¬ ì¤‘...")
                        
                        # ë²”ì£¼í˜• íŠ¹ì„± ì²˜ë¦¬
                        if preprocessing_config.get("categorical_columns"):
                            logger.info(f"ğŸ·ï¸ '{name}' ë°ì´í„°ì…‹ì˜ ë²”ì£¼í˜• íŠ¹ì„± ì²˜ë¦¬ ì¤‘...")
                        
                        # íƒ€ê²Ÿ ì»¬ëŸ¼ ê³„ì‚°
                        if preprocessing_config.get("target_columns"):
                            logger.info(f"ğŸ¯ '{name}' ë°ì´í„°ì…‹ì˜ íƒ€ê²Ÿ íŠ¹ì„± ê³„ì‚° ì¤‘...")
                    
                    # ê³ ê¸‰ íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§ ì ìš©
                    df = pipeline.feature_engineer.engineer_features(df, name, advanced_enabled)
                    
                    engineered_dict[name] = df
                    pbar.update(1)
                
                pbar.close()
                
                # íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§ ì™„ë£Œ ì‹œ íŠ¹ì„± íƒ€ì…ë³„ ê°œìˆ˜ ê³„ì‚°
                engineered_cols_stats = {}
                for name, df in engineered_dict.items():
                    # ì»¬ëŸ¼ íƒ€ì…ë³„ ê°œìˆ˜
                    col_types = {}
                    # í…ìŠ¤íŠ¸ ì„ë² ë”© íŠ¹ì„±
                    text_embedding_cols = len([c for c in df.columns if 'W2V_' in c])
                    # PCA íŠ¹ì„±
                    pca_cols = len([c for c in df.columns if 'PCA_' in c])
                    # TF-IDF íŠ¹ì„±
                    tfidf_cols = len([c for c in df.columns if 'TFIDF_' in c])
                    # ì„ íƒëœ íŠ¹ì„±
                    selected_cols = len([c for c in df.columns if 'selected_' in c])
                    
                    if text_embedding_cols > 0:
                        col_types['ì„ë² ë”© íŠ¹ì„±'] = text_embedding_cols
                    if pca_cols > 0:
                        col_types['PCA íŠ¹ì„±'] = pca_cols
                    if tfidf_cols > 0:
                        col_types['TF-IDF íŠ¹ì„±'] = tfidf_cols
                    if selected_cols > 0:
                        col_types['ì„ íƒëœ íŠ¹ì„±'] = selected_cols
                    
                    engineered_cols_stats[name] = col_types
                
                # íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§ ë‹¨ê³„ ì™„ë£Œ
                feature_metrics = {
                    "ì²˜ë¦¬ëœ ë°ì´í„°ì…‹": len(engineered_dict),
                    "ìµœì¢… íŠ¹ì„± ìˆ˜": sum(len(df.columns) for df in engineered_dict.values()),
                    "ê³ ê¸‰ íŠ¹ì„± ì ìš©": "ì˜ˆ" if advanced_enabled else "ì•„ë‹ˆì˜¤"
                }
                
                # ê³ ê¸‰ íŠ¹ì„± í†µê³„ ì¶”ê°€
                if advanced_enabled:
                    all_text_embedding = sum(stats.get('ì„ë² ë”© íŠ¹ì„±', 0) for stats in engineered_cols_stats.values())
                    all_pca = sum(stats.get('PCA íŠ¹ì„±', 0) for stats in engineered_cols_stats.values())
                    all_tfidf = sum(stats.get('TF-IDF íŠ¹ì„±', 0) for stats in engineered_cols_stats.values())
                    all_selected = sum(stats.get('ì„ íƒëœ íŠ¹ì„±', 0) for stats in engineered_cols_stats.values())
                    
                    if all_text_embedding > 0:
                        feature_metrics['ì„ë² ë”© íŠ¹ì„±'] = all_text_embedding
                    if all_pca > 0:
                        feature_metrics['PCA íŠ¹ì„±'] = all_pca
                    if all_tfidf > 0:
                        feature_metrics['TF-IDF íŠ¹ì„±'] = all_tfidf
                    if all_selected > 0:
                        feature_metrics['ì„ íƒëœ íŠ¹ì„±'] = all_selected
                
                visualizer.complete_step(feature_step, feature_metrics)
                
                # 5. ì…ì°°ê°€ ë¶„ì„ íŠ¹í™” ì²˜ë¦¬
                price_step = visualizer.add_step("ì…ì°°ê°€ íŠ¹í™” ì²˜ë¦¬", "ì…ì°°ê°€ ê´€ë ¨ íŠ¹ì„± ìƒì„± ë° ë¶„ì„")
                
                pbar = visualizer.create_progress_bar(price_step, len(engineered_dict), "ğŸ’° ì…ì°°ê°€ íŠ¹í™” ì²˜ë¦¬ ì§„í–‰")
                for name, df in engineered_dict.items():
                    preprocessing_config = pipeline._get_preprocessing_config(name)
                    if preprocessing_config and "ê¸°ì´ˆê¸ˆì•¡" in df.columns and "ì˜ˆì •ê¸ˆì•¡" in df.columns:
                        # ê°€ê²©ë¹„ìœ¨ ê³„ì‚°
                        if "íˆ¬ì°°ê°€" in df.columns:
                            df["ë‚™ì°°ê°€ê²©ë¹„ìœ¨"] = df["íˆ¬ì°°ê°€"] / df["ì˜ˆì •ê¸ˆì•¡"]
                            logger.info(f"âœ… '{name}' ë°ì´í„°ì…‹ì˜ ë‚™ì°°ê°€ê²©ë¹„ìœ¨ ê³„ì‚° ì™„ë£Œ")
                        
                        # ê¸°íƒ€ ì…ì°°ê°€ ê´€ë ¨ íŠ¹ì„± ìƒì„±
                        pipeline._create_bid_price_features(df, name)
                    
                    engineered_dict[name] = df
                    pbar.update(1)
                
                pbar.close()
                
                # ì…ì°°ê°€ ë¶„ì„ íŠ¹í™” ì²˜ë¦¬ ë‹¨ê³„ ì™„ë£Œ
                price_metrics = {
                    "ì²˜ë¦¬ëœ ë°ì´í„°ì…‹": len(engineered_dict),
                    "ì…ì°°ê°€ íŠ¹ì„± ìƒì„±": sum(1 for df in engineered_dict.values() if "íˆ¬ì°°ê°€_ì˜ˆì •ê¸ˆì•¡ë¹„ìœ¨" in df.columns)
                }
                visualizer.complete_step(price_step, price_metrics)
                
                # ì²˜ë¦¬ëœ ìµœì¢… ë°ì´í„°
                processed_data = engineered_dict
                
                # 6. MongoDBì— ì €ì¥ (ì„ íƒ ì‚¬í•­)
                if not args.skip_upload:
                    upload_step = visualizer.add_step("MongoDB ì—…ë¡œë“œ", "ì „ì²˜ë¦¬ëœ ë°ì´í„°ë¥¼ MongoDBì— ì €ì¥")
                    
                    try:
                        with pipeline.mongodb_handler as mongo:
                            pbar = visualizer.create_progress_bar(upload_step, 1, "ğŸ’¾ MongoDB ì €ì¥ ì§„í–‰")
                            collection_names = mongo.save_datasets(processed_data)
                            pbar.update(1)
                            pbar.close()
                            
                            upload_metrics = {
                                "ì €ì¥ëœ ë°ì´í„°ì…‹": len(collection_names),
                                "ì´ ë¬¸ì„œ ìˆ˜": sum(len(df) for df in processed_data.values())
                            }
                            visualizer.complete_step(upload_step, upload_metrics)
                    except Exception as e:
                        logger.error(f"âŒ MongoDB ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                        visualizer.complete_step(upload_step, {"ì˜¤ë¥˜": str(e)})
                        raise
            
            except Exception as e:
                logger.error(f"íŒŒì´í”„ë¼ì¸ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
                raise
            
            # ê²°ê³¼ ìš”ì•½
            if processed_data:
                summary_step = visualizer.add_step("ê²°ê³¼ ìš”ì•½", "íŒŒì´í”„ë¼ì¸ ì²˜ë¦¬ ê²°ê³¼ ìš”ì•½")
                
                summary_metrics = {
                    "ì²˜ë¦¬ëœ ë°ì´í„°ì…‹": len(processed_data),
                    "ì´ ë ˆì½”ë“œ ìˆ˜": sum(len(df) for df in processed_data.values()),
                    "ì´ íŠ¹ì„± ìˆ˜": sum(len(df.columns) for df in processed_data.values()),
                    "ê³ ê¸‰ íŠ¹ì„± ì ìš©": "ì˜ˆ" if advanced_enabled else "ì•„ë‹ˆì˜¤"
                }
                
                print("\nâœ… ì²˜ë¦¬ ê²°ê³¼ ìš”ì•½:")
                for name, df in processed_data.items():
                    print(f"  - {name}: {df.shape[0]} í–‰ x {df.shape[1]} ì—´")
                    # íŠ¹ì„± ëª©ë¡ (ì²˜ìŒ 5ê°œ)
                    if len(df.columns) > 0:
                        print(f"    ì£¼ìš” íŠ¹ì„±: {', '.join(df.columns[:5])}... ì™¸ {max(0, len(df.columns)-5)}ê°œ")
                    
                    # ê³ ê¸‰ íŠ¹ì„± ì •ë³´ ì¶œë ¥
                    if advanced_enabled and name in engineered_cols_stats:
                        stats = engineered_cols_stats[name]
                        if stats:
                            print(f"    ê³ ê¸‰ íŠ¹ì„±:")
                            for feature_type, count in stats.items():
                                print(f"      - {feature_type}: {count}ê°œ")
                
                visualizer.complete_step(summary_step, summary_metrics)
                
                # MongoDB ê´€ë ¨ ë©”ì‹œì§€
                if args.skip_upload:
                    print("\nâš ï¸ MongoDB ì—…ë¡œë“œë¥¼ ê±´ë„ˆë›°ì—ˆìŠµë‹ˆë‹¤.")
                else:
                    print("\nâœ… MongoDBì— ë°ì´í„°ê°€ ì„±ê³µì ìœ¼ë¡œ ì—…ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.")
                    print("   í™•ì¸ì„ ìœ„í•´ ë‹¤ìŒ ëª…ë ¹ì–´ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:")
                    print("   python src/preprocess_upload_mongo.py --check-only")
                
                # ê³ ê¸‰ ê¸°ëŠ¥ ê´€ë ¨ ë©”ì‹œì§€
                if advanced_enabled:
                    print("\nğŸ” ê³ ê¸‰ ì „ì²˜ë¦¬ ê¸°ëŠ¥ì´ í™œì„±í™”ë˜ì—ˆìŠµë‹ˆë‹¤:")
                    if sum(stats.get('ì„ë² ë”© íŠ¹ì„±', 0) for stats in engineered_cols_stats.values()) > 0:
                        print("   âœ… ì›Œë“œ ì„ë² ë”© (Word2Vec) ê¸°ëŠ¥ì´ ì ìš©ë˜ì—ˆìŠµë‹ˆë‹¤.")
                    if sum(stats.get('PCA íŠ¹ì„±', 0) for stats in engineered_cols_stats.values()) > 0:
                        print("   âœ… PCA ì°¨ì› ì¶•ì†Œê°€ ì ìš©ë˜ì—ˆìŠµë‹ˆë‹¤.")
                    if sum(stats.get('TF-IDF íŠ¹ì„±', 0) for stats in engineered_cols_stats.values()) > 0:
                        print("   âœ… TF-IDF í…ìŠ¤íŠ¸ íŠ¹ì„± ì¶”ì¶œì´ ì ìš©ë˜ì—ˆìŠµë‹ˆë‹¤.")
                    if sum(stats.get('ì„ íƒëœ íŠ¹ì„±', 0) for stats in engineered_cols_stats.values()) > 0:
                        print("   âœ… íŠ¹ì„± ì„ íƒ ì•Œê³ ë¦¬ì¦˜ì´ ì ìš©ë˜ì—ˆìŠµë‹ˆë‹¤.")
            else:
                print("\nâŒ ì²˜ë¦¬í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ë°ì´í„° ë””ë ‰í† ë¦¬ì™€ íŒŒì¼ íŒ¨í„´ì„ í™•ì¸í•˜ì„¸ìš”.")
            
    except Exception as e:
            logger.error(f"íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
            print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
    
    # íŒŒì´í”„ë¼ì¸ ì¢…ë£Œ
    visualizer.end_pipeline()
    
    # ë³´ê³ ì„œ ìƒì„±
    if args.generate_report:
        report_file = visualizer.generate_report({
            "ë°ì´í„° ë””ë ‰í† ë¦¬": args.data_dir or os.getenv('DATA_DIR', './data'),
            "íŒŒì¼ íŒ¨í„´": args.file_pattern,
            "MongoDB ì—…ë¡œë“œ": "ê±´ë„ˆëœ€" if args.skip_upload else "ì™„ë£Œ"
        })
        print(f"\nğŸ“‹ íŒŒì´í”„ë¼ì¸ ë³´ê³ ì„œê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤: {report_file}")
    
    # ì‹œê°í™” í‘œì‹œ
    if args.show_visualization:
        visualizer.visualize_pipeline(show_plot=True, save_plot=True)
    
    logger.info("=== ì…ì°°ê°€ ë°ì´í„° ì „ì²˜ë¦¬ ë° MongoDB ì—…ë¡œë“œ ì™„ë£Œ ===")

def check_mongodb_data(visualizer=None, step_idx=None):
    """
    MongoDBì— ì €ì¥ëœ ë°ì´í„° í™•ì¸
    
    Parameters:
        visualizer (PipelineVisualizer, optional): ì‹œê°í™” ê°ì²´
        step_idx (int, optional): ë‹¨ê³„ ì¸ë±ìŠ¤
        
    Returns:
        dict: ê²€ì‚¬ ê²°ê³¼
    """
    print("\nğŸ” MongoDB ë°ì´í„° í™•ì¸ ì¤‘...")
    
    result = {
        "collections_checked": 0,
        "total_documents": 0,
        "collections_info": {}
    }
    
    try:
        with MongoDBHandler() as mongo:
            # ê¸°ë³¸ ì»¬ë ‰ì…˜ ì´ë¦„ ê°€ì ¸ì˜¤ê¸°
            collection_names = mongo.get_default_collection_names()
            
            # ì§„í–‰ í‘œì‹œì¤„ ìƒì„±
            if visualizer and step_idx is not None:
                pbar = visualizer.create_progress_bar(step_idx, len(collection_names), "MongoDB ì»¬ë ‰ì…˜ ê²€ì‚¬")
            else:
                pbar = None
            
            # ì»¬ë ‰ì…˜ë³„ ë°ì´í„° í™•ì¸
            for key, collection_name in collection_names.items():
                try:
                    # í•´ë‹¹ ì»¬ë ‰ì…˜ ì ‘ê·¼
                    collection = mongo.db[collection_name]
                    
                    # ë¬¸ì„œ ìˆ˜ í™•ì¸
                    count = collection.count_documents({})
                    result["total_documents"] += count
                    result["collections_checked"] += 1
                    
                    if count > 0:
                        # ìƒ˜í”Œ ë°ì´í„° ì¡°íšŒ
                        sample = list(collection.find({}).limit(1))[0]
                        sample_keys = list(sample.keys())
                        
                        # ì»¬ë ‰ì…˜ ì •ë³´ ì €ì¥
                        result["collections_info"][collection_name] = {
                            "document_count": count,
                            "field_count": len(sample_keys),
                            "sample_fields": sample_keys[:5]
                        }
                        
                        # ê³µê³ ë²ˆí˜¸ ë¶„í¬ í™•ì¸ (ìˆëŠ” ê²½ìš°)
                        if 'ê³µê³ ë²ˆí˜¸' in sample_keys:
                            # ê³µê³ ë²ˆí˜¸ ê°œìˆ˜
                            unique_notices = len(collection.distinct('ê³µê³ ë²ˆí˜¸'))
                            result["collections_info"][collection_name]["unique_notices"] = unique_notices
                        
                        # ê²°ê³¼ ì¶œë ¥
                        print(f"\nâœ… ì»¬ë ‰ì…˜: {collection_name}")
                        print(f"  - ë¬¸ì„œ ìˆ˜: {count}ê°œ")
                        print(f"  - í•„ë“œ ìˆ˜: {len(sample_keys)}ê°œ")
                        print(f"  - ì£¼ìš” í•„ë“œ: {', '.join(sample_keys[:5])}... ì™¸ {max(0, len(sample_keys)-5)}ê°œ")
                        
                        # ê³µê³ ë²ˆí˜¸ ë¶„í¬ ì¶œë ¥
                        if 'ê³µê³ ë²ˆí˜¸' in sample_keys:
                            print(f"  - ê³µê³ ë²ˆí˜¸ ìˆ˜: {unique_notices}ê°œ")
                    else:
                        print(f"\nâš ï¸ ì»¬ë ‰ì…˜ {collection_name}ì— ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                        result["collections_info"][collection_name] = {
                            "document_count": 0,
                            "status": "empty"
                        }
                
                except Exception as e:
                    print(f"\nâŒ ì»¬ë ‰ì…˜ {collection_name} í™•ì¸ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                    result["collections_info"][collection_name] = {
                        "status": "error",
                        "error": str(e)
                    }
                
                # ì§„í–‰ í‘œì‹œì¤„ ì—…ë°ì´íŠ¸
                if pbar:
                    pbar.update(1)
            
            # ì§„í–‰ í‘œì‹œì¤„ ë‹«ê¸°
            if pbar:
                pbar.close()
        
        print(f"\nâœ… MongoDB ë°ì´í„° í™•ì¸ ì™„ë£Œ: {result['collections_checked']} ì»¬ë ‰ì…˜, ì´ {result['total_documents']} ë¬¸ì„œ")
    
    except Exception as e:
        logger.error(f"MongoDB ì—°ê²° ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
        print(f"\nâŒ MongoDB ì—°ê²° ì˜¤ë¥˜: {e}")
        result["status"] = "error"
        result["error"] = str(e)
    
    return result

def clear_mongodb_collections(visualizer=None, step_idx=None):
    """
    MongoDB ì»¬ë ‰ì…˜ ì´ˆê¸°í™”
    
    Parameters:
        visualizer (PipelineVisualizer, optional): ì‹œê°í™” ê°ì²´
        step_idx (int, optional): ë‹¨ê³„ ì¸ë±ìŠ¤
        
    Returns:
        dict: ì‚­ì œ ê²°ê³¼
    """
    print("\nğŸ—‘ï¸ MongoDB ì»¬ë ‰ì…˜ ì´ˆê¸°í™” ì¤‘...")
    
    result = {
        "collections_dropped": 0,
        "status": "success",
        "details": {}
    }
    
    try:
        with MongoDBHandler() as mongo:
            # ê¸°ë³¸ ì»¬ë ‰ì…˜ ì´ë¦„ ê°€ì ¸ì˜¤ê¸°
            collection_names = mongo.get_default_collection_names()
            
            # ì§„í–‰ í‘œì‹œì¤„ ìƒì„±
            if visualizer and step_idx is not None:
                pbar = visualizer.create_progress_bar(step_idx, len(collection_names), "MongoDB ì»¬ë ‰ì…˜ ì‚­ì œ")
            else:
                pbar = None
            
            # ê° ì»¬ë ‰ì…˜ ì‚­ì œ
            for key, collection_name in collection_names.items():
                try:
                    mongo.db.drop_collection(collection_name)
                    print(f"  âœ… ì»¬ë ‰ì…˜ {collection_name} ì‚­ì œ ì™„ë£Œ")
                    result["collections_dropped"] += 1
                    result["details"][collection_name] = "dropped"
                except Exception as e:
                    print(f"  âš ï¸ ì»¬ë ‰ì…˜ {collection_name} ì‚­ì œ ì¤‘ ì˜¤ë¥˜: {e}")
                    result["details"][collection_name] = str(e)
                
                # ì§„í–‰ í‘œì‹œì¤„ ì—…ë°ì´íŠ¸
                if pbar:
                    pbar.update(1)
            
            # ì§„í–‰ í‘œì‹œì¤„ ë‹«ê¸°
            if pbar:
                pbar.close()
        
        print(f"\nâœ… MongoDB ì»¬ë ‰ì…˜ ì´ˆê¸°í™” ì™„ë£Œ: {result['collections_dropped']} ì»¬ë ‰ì…˜ ì‚­ì œë¨")
    
    except Exception as e:
        logger.error(f"MongoDB ì—°ê²° ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
        print(f"\nâŒ MongoDB ì—°ê²° ì˜¤ë¥˜: {e}")
        result["status"] = "error"
        result["error"] = str(e)
    
    return result

if __name__ == "__main__":
    main() 