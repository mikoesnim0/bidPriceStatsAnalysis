# ë°ì´í„° íŒŒì´í”„ë¼ì¸

## í•¨ìˆ˜ëª©ë¡

- transform
- restructure_data
- parse_sajeong_rate
- group_to_list
- separate_data
- process_data
- log_transforming
- normalizing
- one_hot_encoding
- load_bge_model
- embedding
- get_embedding_vector
- series_to_dataframe
- dimension_reducing_PCA
- dimension_reducing_UMAP
- calculate_target
- parse_range_level
- load_bins
- generate_bins
- extract_bounds
- process_row_fixed_bins
- data_to_target


### ë¡œê·¸ ë³€í™˜

# ============================================================
#    ë¡œê·¸ ë³€í™˜ í•¨ìˆ˜: Series ê°’ì— log1pë¥¼ ì ìš©
# ============================================================

def log_transforming(series):
    """
    ì£¼ì–´ì§„ Pandas Seriesì˜ ê°’ì„ ìì—°ë¡œê·¸ ë³€í™˜(log1p)ì„ ìˆ˜í–‰í•˜ì—¬ ë°˜í™˜í•©ë‹ˆë‹¤.

    Parameters:
        series (pd.Series): ë¡œê·¸ ë³€í™˜í•  ë°ì´í„°. ëª¨ë“  ê°’ì€ 0 ì´ìƒì˜ ì–‘ìˆ˜ì—¬ì•¼ í•©ë‹ˆë‹¤.

    Returns:
        pd.Series: ì›ë³¸ ì¸ë±ìŠ¤ì™€ ì´ë¦„ì„ ìœ ì§€í•œ ë¡œê·¸ ë³€í™˜ëœ ë°ì´í„°.

    Raises:
        TypeError: ì…ë ¥ê°’ì´ Pandas Seriesê°€ ì•„ë‹ ê²½ìš°.
        ValueError: Seriesì— ìŒìˆ˜ ê°’ì´ í¬í•¨ëœ ê²½ìš°.
    """
    # import pandas as pd
    # import numpy as np
    # import logging

    logging.info(f"âœ… [log_transforming] ë¡œê·¸ ë³€í™˜ ì‹œì‘ - Column: {series.name}")

    # ì…ë ¥ ë°ì´í„° íƒ€ì… í™•ì¸
    if not isinstance(series, pd.Series):
        raise TypeError("ì…ë ¥ê°’ì€ pandas Seriesì—¬ì•¼ í•©ë‹ˆë‹¤.")

    # ìˆ«ìí˜• ë°ì´í„°ì¸ì§€ í™•ì¸ (ë¶ˆí•„ìš”í•œ ë³€í™˜ ë°©ì§€)
    if not np.issubdtype(series.dtype, np.number):
        raise ValueError("ë¡œê·¸ ë³€í™˜ì„ ìœ„í•´ ìˆ«ìí˜• ë°ì´í„°ê°€ í•„ìš”í•©ë‹ˆë‹¤.")

    # ìŒìˆ˜ ê°’ í™•ì¸
    if (series < 0).any():
        raise ValueError("ë¡œê·¸ ë³€í™˜í•  ë°ì´í„°ì— ìŒìˆ˜ ê°’ì´ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")

    # float íƒ€ì…ì´ ì•„ë‹Œ ê²½ìš°ë§Œ ë³€í™˜
    if not np.issubdtype(series.dtype, np.floating):
        series = series.astype(float)

    # ìì—°ë¡œê·¸ ë³€í™˜
    result_series = np.log1p(series)

    logging.info(f"âœ… [log_transforming] ë¡œê·¸ ë³€í™˜ ì™„ë£Œ - ê²°ê³¼ ìƒ˜í”Œ: {result_series.head(3).tolist()}")

    return result_series


### ì •ê·œí™”

def normalizing(series):
    """
    ì£¼ì–´ì§„ Pandas Seriesì˜ ê°’ì„ í‘œì¤€í™”(Standardization)í•˜ì—¬ í‰ê·  0, í‘œì¤€í¸ì°¨ 1ì˜ ë¶„í¬ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.

    Parameters:
        series (pd.Series): í‘œì¤€í™”í•  ë°ì´í„°.

    Returns:
        pd.Series: í‘œì¤€í™”ëœ ë°ì´í„° (ì›ë³¸ ì¸ë±ìŠ¤ì™€ ì´ë¦„ ìœ ì§€).

    Raises:
        TypeError: ì…ë ¥ê°’ì´ Pandas Seriesê°€ ì•„ë‹ ê²½ìš°.
    """
    # import pandas as pd
    # import numpy as np
    # import logging
    from sklearn.preprocessing import StandardScaler

    logging.info(f"âœ… [normalizing] í‘œì¤€í™” ì‹œì‘ - Column: {series.name}")

    if not isinstance(series, pd.Series):
        logging.error("âŒì…ë ¥ê°’ì´ pandas Seriesê°€ ì•„ë‹™ë‹ˆë‹¤!")
        raise TypeError("ì…ë ¥ê°’ì€ pandas Seriesì—¬ì•¼ í•©ë‹ˆë‹¤.")

    scaler = StandardScaler()

    # 1ì°¨ì› ë°ì´í„°ë¥¼ 2ì°¨ì› ë°°ì—´ë¡œ ë³€í™˜ í›„ í‘œì¤€í™”
    result_array = scaler.fit_transform(series.values.reshape(-1, 1)).flatten()

    # ì›ë³¸ ì¸ë±ìŠ¤ ë° ì»¬ëŸ¼ëª… ìœ ì§€
    result_series = pd.Series(result_array, index=series.index, name=series.name)

    logging.info(f"âœ… [normalizing] í‘œì¤€í™” ì™„ë£Œ - ê²°ê³¼ ìƒ˜í”Œ: {result_series.head(3).tolist()}")

    return result_series


### ì›-í•« ì¸ì½”ë”©

def one_hot_encoding(series, delimiter="/"):
    """
    ì£¼ì–´ì§„ Pandas Seriesì— ëŒ€í•´ ì§€ì •ëœ êµ¬ë¶„ì(delimiter)ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì›-í•« ì¸ì½”ë”©ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.

    Parameters:
        series (pd.Series): ì›ë³¸ ë¬¸ìì—´ ë°ì´í„°. ê° ì…€ì— ì—¬ëŸ¬ ê°’ì´ í¬í•¨ë˜ì–´ ìˆì„ ìˆ˜ ìˆìŒ.
        delimiter (str): ê°’ë“¤ ê°„ì˜ êµ¬ë¶„ì (ê¸°ë³¸ê°’: "/").

    Returns:
        pd.DataFrame: ì›-í•« ì¸ì½”ë”© ê²°ê³¼ë¥¼ í¬í•¨í•˜ëŠ” DataFrame.

    Raises:
        TypeError: ì…ë ¥ê°’ì´ Pandas Seriesê°€ ì•„ë‹ ê²½ìš°.
        ValueError: Seriesì˜ ë°ì´í„° íƒ€ì…ì´ ë¬¸ìì—´ì´ ì•„ë‹ ê²½ìš°.
    """
    # import pandas as pd
    # import logging

    logging.info(f"âœ… [one_hot_encoding] ì‹œì‘ - Column: {series.name}")

    if not isinstance(series, pd.Series):
        logging.error("âŒ ì…ë ¥ê°’ì´ pandas Seriesê°€ ì•„ë‹™ë‹ˆë‹¤!")
        raise TypeError("âŒ ì…ë ¥ê°’ì€ pandas Seriesì—¬ì•¼ í•©ë‹ˆë‹¤!")

    # ë¬¸ìì—´ íƒ€ì… í™•ì¸
    if not series.dtype == "object":
        logging.error("âŒ ì›-í•« ì¸ì½”ë”©ì„ ìœ„í•´ SeriesëŠ” ë¬¸ìì—´ íƒ€ì…ì´ì–´ì•¼ í•©ë‹ˆë‹¤!")
        raise ValueError("âŒ ì›-í•« ì¸ì½”ë”©ì„ ìœ„í•´ SeriesëŠ” ë¬¸ìì—´ íƒ€ì…ì´ì–´ì•¼ í•©ë‹ˆë‹¤!")

    # ë¬¸ìì—´ì„ delimiterë¡œ ë¶„ë¦¬í•˜ì—¬ dummies ìƒì„±
    result_df = series.str.get_dummies(sep=delimiter)

    # ë¹ˆ ë¬¸ìì—´ ì»¬ëŸ¼ì´ ìƒì„±ë  ê²½ìš° ì œê±°
    if "" in result_df.columns:
        result_df = result_df.drop(columns="")

    # ê¸°ì¡´ ì»¬ëŸ¼ëª…ì„ ì ‘ë‘ì‚¬ë¡œ ì¶”ê°€í•˜ì—¬ ê²°ê³¼ DataFrame ìƒì„±
    result_df = result_df.add_prefix(f"{series.name}_")

    # âœ… ê²°ê³¼ ì»¬ëŸ¼ ìˆ˜ & ì»¬ëŸ¼ ë¦¬ìŠ¤íŠ¸ ì¶œë ¥
    logging.info(f"âœ… [one_hot_encoding] ì™„ë£Œ - ìƒì„±ëœ ì»¬ëŸ¼ ìˆ˜: {len(result_df.columns)}, ì»¬ëŸ¼ ëª©ë¡: {result_df.columns.tolist()}")

    return result_df


### í…ìŠ¤íŠ¸ ì„ë² ë”©

def load_bge_model(model_name="BAAI/bge-m3"):
    """
    ì§€ì •ëœ ëª¨ë¸ ì´ë¦„ì˜ BGE-M3 ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.

    Parameters:
        model_name (str, optional): ì‚¬ìš©í•  ëª¨ë¸ ì´ë¦„ (ê¸°ë³¸ê°’: "BAAI/bge-m3").

    Returns:
        tuple: (tokenizer, model, device) - ë¡œë“œëœ í† í¬ë‚˜ì´ì €, ëª¨ë¸, ì‹¤í–‰ ë””ë°”ì´ìŠ¤ (GPU/CPU).

    Raises:
        RuntimeError: ëª¨ë¸ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí•œ ê²½ìš°.
    """
    # import logging
    import torch
    from transformers import AutoTokenizer, AutoModel

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    logging.info(f"âœ… [load_bge_model] ëª¨ë¸ ë¡œë“œ ì‹œì‘: {model_name}")

    try:
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        model = AutoModel.from_pretrained(model_name)
        model.to(device)  # ëª¨ë¸ì„ GPU/CPUì— ë¡œë“œ
        model.eval()  # í‰ê°€ ëª¨ë“œ ì„¤ì •
        logging.info(f"âœ… [load_bge_model] ëª¨ë¸ '{model_name}'ì´(ê°€) {device}ì—ì„œ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œë¨")

    except Exception as e:
        logging.error(f"âŒ ëª¨ë¸ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        raise RuntimeError(f"âŒ ëª¨ë¸ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

    return tokenizer, model, device


def get_embedding_vector(texts, tokenizer, model, device, max_length=512):
    """
    í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°›ì•„ BGE-M3 ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì„ë² ë”© ë²¡í„° ë°°ì—´ì„ ìƒì„±í•©ë‹ˆë‹¤.

    Parameters:
        texts (list of str): ì„ë² ë”©í•  í…ìŠ¤íŠ¸ë“¤ì˜ ë¦¬ìŠ¤íŠ¸.
        tokenizer: ë¡œë“œëœ í† í¬ë‚˜ì´ì €.
        model: ë¡œë“œëœ ì„ë² ë”© ëª¨ë¸.
        device: ì‹¤í–‰ ë””ë°”ì´ìŠ¤ (GPU/CPU).
        max_length (int, optional): ìµœëŒ€ í† í° ê¸¸ì´ (ê¸°ë³¸ê°’: 512).

    Returns:
        np.ndarray: (ë°°ì¹˜ í¬ê¸°, ì„ë² ë”© ì°¨ì›) í˜•íƒœì˜ ì„ë² ë”© ë²¡í„° ë°°ì—´.

    Raises:
        TypeError: textsê°€ ë¬¸ìì—´ ë¦¬ìŠ¤íŠ¸ê°€ ì•„ë‹ ê²½ìš°.
    """
    # import numpy as np
    # import pandas as pd
    import torch
    import logging

    logging.info(f"âœ… [get_embedding_vector] í…ìŠ¤íŠ¸ ê°œìˆ˜: {len(texts)}")

    if not isinstance(texts, list):
        logging.error("âŒ ì…ë ¥ê°’ì´ ë¬¸ìì—´ ë¦¬ìŠ¤íŠ¸ê°€ ì•„ë‹™ë‹ˆë‹¤!")
        raise TypeError("âŒ ì…ë ¥ê°’ì€ ë¬¸ìì—´ ë¦¬ìŠ¤íŠ¸ì—¬ì•¼ í•©ë‹ˆë‹¤!")

    if not texts:
        logging.warning("âš ï¸ [get_embedding_vector] ë¹ˆ ë¦¬ìŠ¤íŠ¸ ì…ë ¥ë¨. ë¹ˆ ë°°ì—´ ë°˜í™˜.")
        return np.array([])

    # ê²°ì¸¡ê°’ì„ ë¹ˆ ë¬¸ìì—´ë¡œ ë³€í™˜
    texts = [str(text) if pd.notnull(text) else "" for text in texts]

    # í…ìŠ¤íŠ¸ í† í°í™” ë° ëª¨ë¸ ì…ë ¥ ë°ì´í„° ì¤€ë¹„
    inputs = tokenizer(texts, return_tensors="pt", truncation=True, padding=True, max_length=max_length)
    inputs = {key: value.to(device) for key, value in inputs.items()}

    with torch.no_grad():
        outputs = model(**inputs)

    # CLS í† í° ì„ë² ë”© ì¶”ì¶œ
    embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()

    logging.info(f"âœ… [get_embedding_vector] ì„ë² ë”© ì™„ë£Œ - ê²°ê³¼ shape: {embeddings.shape}")

    return embeddings


def embedding(series, tokenizer, model, device, batch_size=32):
    """
    ì£¼ì–´ì§„ Pandas Seriesì˜ í…ìŠ¤íŠ¸ë¥¼ ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì„ë² ë”©í•˜ì—¬,
    ê° í…ìŠ¤íŠ¸ì— í•´ë‹¹í•˜ëŠ” ì„ë² ë”© ë²¡í„° ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.

    Parameters:
        series (pd.Series): ì„ë² ë”©í•  í…ìŠ¤íŠ¸ ë°ì´í„°ê°€ í¬í•¨ëœ ì»¬ëŸ¼.
        tokenizer: ë¡œë“œëœ í† í¬ë‚˜ì´ì €.
        model: ë¡œë“œëœ ì„ë² ë”© ëª¨ë¸.
        device: ì‹¤í–‰ ë””ë°”ì´ìŠ¤ (GPU/CPU).
        batch_size (int, optional): ë°°ì¹˜ í¬ê¸° (ê¸°ë³¸ê°’: 32).

    Returns:
        pd.Series: ê° í…ìŠ¤íŠ¸ì˜ ì„ë² ë”© ë²¡í„°(ë¦¬ìŠ¤íŠ¸)ë¥¼ í¬í•¨í•˜ëŠ” Series.

    Raises:
        TypeError: ì…ë ¥ê°’ì´ Pandas Seriesê°€ ì•„ë‹ ê²½ìš°.
    """
    # import numpy as np
    # import pandas as pd
    # import logging
    import torch
    from tqdm import tqdm

    logging.info(f"âœ… [embedding] ì‹œì‘ - Column: {series.name}, ê¸¸ì´: {len(series)}")

    if not isinstance(series, pd.Series):
        logging.error("âŒ ì…ë ¥ ê°’ì´ Pandas Seriesê°€ ì•„ë‹™ë‹ˆë‹¤!")
        raise TypeError("âŒ ì…ë ¥ ê°’ì€ Pandas Seriesì—¬ì•¼ í•©ë‹ˆë‹¤!")

    if series.empty:
        logging.warning("âš ï¸ [embedding] ë¹ˆ Series ì…ë ¥ë¨. ë¹ˆ ê²°ê³¼ ë°˜í™˜.")
        return pd.Series([], index=series.index, dtype=object)

    # ê²°ì¸¡ê°’ì„ ë¹ˆ ë¬¸ìì—´ë¡œ ëŒ€ì²´
    series = series.fillna("")

    embeddings = []
    num_batches = (len(series) + batch_size - 1) // batch_size

    for i in tqdm(range(0, len(series), batch_size), total=num_batches, desc="ì„ë² ë”© ì§„í–‰ ì¤‘", position=0):
        batch_texts = series.iloc[i:i + batch_size].tolist()
        batch_embeddings = get_embedding_vector(batch_texts, tokenizer, model, device)

        if batch_embeddings is None or len(batch_embeddings) == 0:
            logging.warning(f"âš ï¸ [embedding] {i}ë²ˆì§¸ ë°°ì¹˜ì—ì„œ ë¹ˆ ê²°ê³¼ ë°œìƒ. ë¹ˆ ë°°ì—´ ì¶”ê°€.")
            batch_embeddings = np.zeros((len(batch_texts), model.config.hidden_size))

        embeddings.extend(batch_embeddings)

    # ì„ë² ë”© ë²¡í„°ë¥¼ ë¦¬ìŠ¤íŠ¸ í˜•ì‹ìœ¼ë¡œ ì €ì¥
    result_series = pd.Series([emb.tolist() for emb in embeddings], index=series.index, dtype=object)

    logging.info("âœ… [embedding] ì™„ë£Œ")

    return result_series


### ì°¨ì›ì¶•ì†Œ

# ============================================================
#  SVD ì°¨ì› ì¶•ì†Œ í•¨ìˆ˜: Truncated SVDë¥¼ ì‚¬ìš©í•œ ì°¨ì› ì¶•ì†Œ
# ============================================================
def dimension_reducing_SVD(df, prefix, components_n=2, random_state=42):
    """
    ì£¼ì–´ì§„ DataFrameì„ Truncated SVDë¥¼ ì‚¬ìš©í•˜ì—¬ ì°¨ì› ì¶•ì†Œí•©ë‹ˆë‹¤.

    Parameters:
        df (pd.DataFrame): ì°¨ì› ì¶•ì†Œí•  ì…ë ¥ ë°ì´í„°.
        prefix (str): ê²°ê³¼ ì»¬ëŸ¼ ì´ë¦„ì— ì‚¬ìš©í•  ì ‘ë‘ì‚¬.
        components_n (int, optional): ëª©í‘œ ì°¨ì› ìˆ˜ (ê¸°ë³¸ê°’: 2).
        random_state (int, optional): SVDì˜ ëœë¤ ì‹œë“œ ê°’ (ê¸°ë³¸ê°’: 42).

    Returns:
        pd.DataFrame: SVD ì°¨ì› ì¶•ì†Œ í›„ ê²°ê³¼ DataFrame (ì›ë³¸ ì¸ë±ìŠ¤ ìœ ì§€).

    Raises:
        TypeError: ì…ë ¥ê°’ì´ Pandas DataFrameì´ ì•„ë‹ ê²½ìš°.
        ValueError: components_nì´ 1ë³´ë‹¤ ì‘ì„ ê²½ìš°.
    """
    # import pandas as pd
    # import logging
    from sklearn.decomposition import TruncatedSVD

    logging.info(f"âœ… [dimension_reducing_SVD] ì‹œì‘ - ì…ë ¥ DataFrame shape: {df.shape}")

    # ì…ë ¥ íƒ€ì… ê²€ì¦
    if not isinstance(df, pd.DataFrame):
        logging.error("âŒ ì…ë ¥ê°’ì€ pandas DataFrameì´ì–´ì•¼ í•©ë‹ˆë‹¤!")
        raise TypeError("âŒ ì…ë ¥ê°’ì€ pandas DataFrameì´ì–´ì•¼ í•©ë‹ˆë‹¤!")

    # components_n ê°’ ê²€ì¦
    if components_n < 1:
        logging.error("âŒ components_n ê°’ì€ 1 ì´ìƒì´ì–´ì•¼ í•©ë‹ˆë‹¤!")
        raise ValueError("âŒ components_n ê°’ì€ 1 ì´ìƒì´ì–´ì•¼ í•©ë‹ˆë‹¤!")

    if components_n > df.shape[1]:
        components_n = df.shape[1]
        logging.warning("âš ï¸ [PCA] components_nì´ ì…ë ¥ ì°¨ì›ë³´ë‹¤ í¬ë¯€ë¡œ ì…ë ¥ ì°¨ì›ìœ¼ë¡œ ì„¤ì •í•©ë‹ˆë‹¤.")

    # Truncated SVD ì‹¤í–‰
    svd = TruncatedSVD(n_components=components_n, random_state=random_state)
    reduced_data = svd.fit_transform(df)

    # ê²°ê³¼ ì»¬ëŸ¼ëª… ìƒì„±
    reduced_columns = [f"SVD_{prefix}_{i + 1}" for i in range(components_n)]

    # ê²°ê³¼ DataFrame ìƒì„± (ì›ë³¸ ì¸ë±ìŠ¤ ìœ ì§€)
    result_df = pd.DataFrame(reduced_data, columns=reduced_columns, index=df.index)

    logging.info(f"âœ… [dimension_reducing_SVD] ì™„ë£Œ - ê²°ê³¼ shape: {result_df.shape}")

    return result_df


# ============================================================
#  PCA ì°¨ì› ì¶•ì†Œ í•¨ìˆ˜: PCAë¥¼ ì‚¬ìš©í•œ ì°¨ì› ì¶•ì†Œ
# ============================================================
def dimension_reducing_PCA(df, prefix, components_n=2):
    """
    ì£¼ì–´ì§„ DataFrameì„ PCAë¥¼ ì‚¬ìš©í•˜ì—¬ ì°¨ì› ì¶•ì†Œí•©ë‹ˆë‹¤.

    Parameters:
        df (pd.DataFrame): ì°¨ì› ì¶•ì†Œí•  ì…ë ¥ ë°ì´í„°.
        prefix (str): ê²°ê³¼ ì»¬ëŸ¼ ì´ë¦„ì— ì‚¬ìš©í•  ì ‘ë‘ì‚¬.
        components_n (int, optional): ëª©í‘œ ì°¨ì› ìˆ˜ (ê¸°ë³¸ê°’: 2).

    Returns:
        pd.DataFrame: PCA ì°¨ì› ì¶•ì†Œ í›„ ê²°ê³¼ DataFrame (ì›ë³¸ ì¸ë±ìŠ¤ ìœ ì§€).

    Raises:
        TypeError: ì…ë ¥ê°’ì´ Pandas DataFrameì´ ì•„ë‹ ê²½ìš°.
        ValueError: components_nì´ 1ë³´ë‹¤ ì‘ì„ ê²½ìš°.
    """
    # import pandas as pd
    # import logging
    from sklearn.decomposition import PCA

    logging.info(f"âœ… [dimension_reducing_PCA] ì‹œì‘ - ì…ë ¥ DataFrame shape: {df.shape}")

    # ì…ë ¥ íƒ€ì… ê²€ì¦
    if not isinstance(df, pd.DataFrame):
        logging.error("âŒ ì…ë ¥ê°’ì€ pandas DataFrameì´ì–´ì•¼ í•©ë‹ˆë‹¤!")
        raise TypeError("âŒ ì…ë ¥ê°’ì€ pandas DataFrameì´ì–´ì•¼ í•©ë‹ˆë‹¤!")

    # components_n ê°’ ê²€ì¦
    if components_n < 1:
        logging.error("âŒ components_n ê°’ì€ 1 ì´ìƒì´ì–´ì•¼ í•©ë‹ˆë‹¤!")
        raise ValueError("âŒ components_n ê°’ì€ 1 ì´ìƒì´ì–´ì•¼ í•©ë‹ˆë‹¤!")

    if components_n > df.shape[1]:
        components_n = df.shape[1]
        logging.warning("âš ï¸ [PCA] components_nì´ ì…ë ¥ ì°¨ì›ë³´ë‹¤ í¬ë¯€ë¡œ ì…ë ¥ ì°¨ì›ìœ¼ë¡œ ì„¤ì •í•©ë‹ˆë‹¤.")

    # PCA ì‹¤í–‰
    pca = PCA(n_components=components_n)
    reduced_data = pca.fit_transform(df)

    # ê²°ê³¼ ì»¬ëŸ¼ëª… ìƒì„±
    reduced_columns = [f"PCA_{prefix}_{i + 1}" for i in range(components_n)]

    # ì„¤ëª…ëœ ë¶„ì‚° ê³„ì‚°
    explained_variance = sum(pca.explained_variance_ratio_) * 100

    # ê²°ê³¼ DataFrame ìƒì„± (ì›ë³¸ ì¸ë±ìŠ¤ ìœ ì§€)
    result_df = pd.DataFrame(reduced_data, columns=reduced_columns, index=df.index)

    logging.info(f"âœ… [dimension_reducing_PCA] ì™„ë£Œ - ê²°ê³¼ shape: {result_df.shape}, ì„¤ëª…ëœ ë¶„ì‚°: {explained_variance:.2f}%")

    return result_df


# ============================================================
# UMAP ì°¨ì› ì¶•ì†Œ í•¨ìˆ˜: UMAPì„ ì‚¬ìš©í•œ ë¹„ì„ í˜• ì°¨ì› ì¶•ì†Œ
# ============================================================
def dimension_reducing_UMAP(df, prefix, components_n=2, n_neighbors=15, metric="euclidean", random_state=42):
    """
    ì£¼ì–´ì§„ DataFrameì„ UMAPì„ ì‚¬ìš©í•˜ì—¬ ë¹„ì„ í˜• ì°¨ì› ì¶•ì†Œí•©ë‹ˆë‹¤.

    Parameters:
        df (pd.DataFrame): ì°¨ì› ì¶•ì†Œí•  ì…ë ¥ ë°ì´í„°.
        prefix (str): ê²°ê³¼ ì»¬ëŸ¼ ì´ë¦„ì— ì‚¬ìš©í•  ì ‘ë‘ì‚¬.
        components_n (int, optional): ëª©í‘œ ì°¨ì› ìˆ˜ (ê¸°ë³¸ê°’: 2).
        n_neighbors (int, optional): UMAPì˜ ì´ì›ƒ ìˆ˜ (ê¸°ë³¸ê°’: 15).
        metric (str, optional): ê±°ë¦¬ ì¸¡ì • ë°©ì‹ (ê¸°ë³¸ê°’: "euclidean").
        random_state (int, optional): ëœë¤ ì‹œë“œ ê°’ (ê¸°ë³¸ê°’: 42).

    Returns:
        pd.DataFrame: UMAP ì°¨ì› ì¶•ì†Œ í›„ ê²°ê³¼ DataFrame (ì›ë³¸ ì¸ë±ìŠ¤ ìœ ì§€).

    Raises:
        TypeError: ì…ë ¥ê°’ì´ Pandas DataFrameì´ ì•„ë‹ ê²½ìš°.
        ValueError: components_nì´ 1ë³´ë‹¤ ì‘ì„ ê²½ìš°.
    """
    # import pandas as pd
    # import logging
    import umap

    logging.info(f"âœ…[dimension_reducing_UMAP] ì‹œì‘ - ì…ë ¥ DataFrame shape: {df.shape}")

    # ì…ë ¥ íƒ€ì… ê²€ì¦
    if not isinstance(df, pd.DataFrame):
        logging.error("âŒì…ë ¥ê°’ì€ pandas DataFrameì´ì–´ì•¼ í•©ë‹ˆë‹¤!")
        raise TypeError("âŒì…ë ¥ê°’ì€ pandas DataFrameì´ì–´ì•¼ í•©ë‹ˆë‹¤!")

    # components_n ê°’ ê²€ì¦
    if components_n < 1:
        logging.error("âŒcomponents_n ê°’ì€ 1 ì´ìƒì´ì–´ì•¼ í•©ë‹ˆë‹¤!")
        raise ValueError("âŒcomponents_n ê°’ì€ 1 ì´ìƒì´ì–´ì•¼ í•©ë‹ˆë‹¤!")

    if components_n > df.shape[1]:
        components_n = df.shape[1]
        logging.warning("âš ï¸ [PCA] components_nì´ ì…ë ¥ ì°¨ì›ë³´ë‹¤ í¬ë¯€ë¡œ ì…ë ¥ ì°¨ì›ìœ¼ë¡œ ì„¤ì •í•©ë‹ˆë‹¤.")

    # UMAP ì‹¤í–‰
    umap_model = umap.UMAP(n_components=components_n, n_neighbors=n_neighbors, metric=metric, random_state=random_state)
    reduced_data = umap_model.fit_transform(df)

    # ê²°ê³¼ ì»¬ëŸ¼ëª… ìƒì„±
    reduced_columns = [f"UMAP_{prefix}_{i + 1}" for i in range(components_n)]

    # ê²°ê³¼ DataFrame ìƒì„± (ì›ë³¸ ì¸ë±ìŠ¤ ìœ ì§€)
    result_df = pd.DataFrame(reduced_data, columns=reduced_columns, index=df.index)

    logging.info(f"âœ…[dimension_reducing_UMAP] ì™„ë£Œ - ê²°ê³¼ shape: {result_df.shape}")

    return result_df


### ë°ì´í„° ë¶„ë¦¬, ê°€ê³µ

# ============================================================
#   ë°ì´í„° ë¶„ë¦¬ í•¨ìˆ˜: 'ì˜ˆê°€ë²”ìœ„' ê¸°ì¤€ ê·¸ë£¹ ë¶„ë¥˜
# ============================================================
def separate_data(df):
    """
    'ì˜ˆê°€ë²”ìœ„' ì»¬ëŸ¼ì„ ê¸°ì¤€ìœ¼ë¡œ ë°ì´í„°ë¥¼ ì„¸ ê·¸ë£¹(ì˜ˆ: range3, range2, others)ìœ¼ë¡œ ë¶„ë¥˜í•©ë‹ˆë‹¤.

    Parameters:
        df (pd.DataFrame): ì…ë ¥ ë°ì´í„°. 'ì˜ˆê°€ë²”ìœ„' ì»¬ëŸ¼ì´ í¬í•¨ë˜ì–´ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.

    Returns:
        dict: {'range3': DataFrame, 'range2': DataFrame, 'others': DataFrame} í˜•íƒœë¡œ ê·¸ë£¹ë³„ DataFrame ë°˜í™˜.

    Raises:
        TypeError: ì…ë ¥ ë°ì´í„°ê°€ Pandas DataFrameì´ ì•„ë‹ ê²½ìš°.
        KeyError: 'ì˜ˆê°€ë²”ìœ„' ì»¬ëŸ¼ì´ DataFrameì— ì—†ì„ ê²½ìš°.
    """
    # import pandas as pd
    # import logging

    logging.info(f"âœ…[separate_data] ì‹œì‘ - ì…ë ¥ DataFrame shape: {df.shape}")

    # ì…ë ¥ íƒ€ì… ê²€ì¦
    if not isinstance(df, pd.DataFrame):
        logging.error("âŒì…ë ¥ ë°ì´í„°ëŠ” pandas DataFrameì´ì–´ì•¼ í•©ë‹ˆë‹¤!")
        raise TypeError("âŒì…ë ¥ ë°ì´í„°ëŠ” pandas DataFrameì´ì–´ì•¼ í•©ë‹ˆë‹¤!")

    # 'ì˜ˆê°€ë²”ìœ„' ì»¬ëŸ¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
    if "ì˜ˆê°€ë²”ìœ„" not in df.columns:
        logging.error("âŒ'ì˜ˆê°€ë²”ìœ„' ì»¬ëŸ¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤!")
        raise KeyError("âŒ'ì˜ˆê°€ë²”ìœ„' ì»¬ëŸ¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤!")

    # ì§€ì •í•œ ì˜ˆê°€ë²”ìœ„ ê°’ì„ ê¸°ì¤€ìœ¼ë¡œ ê·¸ë£¹í™”
    range_values = ["+3%~-3%", "+2%~-2%"]
    range3_df = df[df["ì˜ˆê°€ë²”ìœ„"] == "+3%~-3%"].reset_index(drop=True)
    range2_df = df[df["ì˜ˆê°€ë²”ìœ„"] == "+2%~-2%"].reset_index(drop=True)
    others_df = df[~df["ì˜ˆê°€ë²”ìœ„"].isin(range_values)].reset_index(drop=True)

    logging.info(
        f"âœ…[separate_data] ë¶„ë¥˜ ì™„ë£Œ - range3: {len(range3_df)}, range2: {len(range2_df)}, others: {len(others_df)}")

    return {"range3": range3_df, "range2": range2_df, "others": others_df}


# ============================================================
#   ë°ì´í„° ì¬êµ¬ì„± í•¨ìˆ˜: ê³µê³  ë°ì´í„°ì™€ íˆ¬ì°° ë°ì´í„° ë³‘í•© ë° ì •ë¦¬
# ============================================================
def restructure_data(df1, df2):
    """
    ê³µê³  ë°ì´í„°ì™€ íˆ¬ì°° ë°ì´í„°ë¥¼ ì •ë¦¬ í•œ í›„ ë³‘í•© ë° ì¬êµ¬ì„±í•©ë‹ˆë‹¤.

    Parameters:
        df1 (pd.DataFrame): ê³µê³  ë°ì´í„°.
        df2 (pd.DataFrame): íˆ¬ì°° ë°ì´í„°.

    Returns:
        pd.DataFrame: ë³‘í•©ëœ ìµœì¢… ë°ì´í„°.

    Raises:
        TypeError: ì…ë ¥ ë°ì´í„°ê°€ Pandas DataFrameì´ ì•„ë‹ ê²½ìš°.
        KeyError: í•„ìš”í•œ ì»¬ëŸ¼ì´ ì¡´ì¬í•˜ì§€ ì•Šì„ ê²½ìš°.
    """
    # import pandas as pd
    # import logging

    logging.info("âœ… [restructure_data] ë°ì´í„° ì¬êµ¬ì„± ì‹œì‘...")

    # ì…ë ¥ ë°ì´í„° ê²€ì¦
    if not isinstance(df1, pd.DataFrame) or not isinstance(df2, pd.DataFrame):
        logging.error("âŒ ì…ë ¥ ë°ì´í„°ëŠ” Pandas DataFrameì´ì–´ì•¼ í•©ë‹ˆë‹¤!")
        raise TypeError("âŒ ì…ë ¥ ë°ì´í„°ëŠ” Pandas DataFrameì´ì–´ì•¼ í•©ë‹ˆë‹¤!")

    # ê³µê³ ë²ˆí˜¸ ì»¬ëŸ¼ í™•ì¸
    if "ê³µê³ ë²ˆí˜¸" not in df1.columns or "ê³µê³ ë²ˆí˜¸" not in df2.columns:
        logging.error("âŒ 'ê³µê³ ë²ˆí˜¸' ì»¬ëŸ¼ì´ ë‘ ë°ì´í„°í”„ë ˆì„ì— ëª¨ë‘ ì¡´ì¬í•´ì•¼ í•©ë‹ˆë‹¤!")
        raise KeyError("âŒ 'ê³µê³ ë²ˆí˜¸' ì»¬ëŸ¼ì´ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤.")

    # í•„ìš”í•œ ì»¬ëŸ¼ ì •ì˜ ë° ì¡´ì¬ ì—¬ë¶€ í™•ì¸
    df1_required_columns = ['ê³µê³ ë²ˆí˜¸', 'ê³µê³ ì œëª©', 'ë°œì£¼ì²˜(ìˆ˜ìš”ê¸°ê´€)', 'ì§€ì—­ì œí•œ', 'ê¸°ì´ˆê¸ˆì•¡',
                            'ì˜ˆì •ê°€ê²©', 'ì˜ˆê°€ë²”ìœ„', 'Aê°’', 'íˆ¬ì°°ë¥ (%)', 'ì°¸ì—¬ì—…ì²´ìˆ˜', 'ê³µê³ êµ¬ë¶„í‘œì‹œ']
    df2_required_columns = ['ê³µê³ ë²ˆí˜¸', 'ê¸°ì´ˆëŒ€ë¹„ ì‚¬ì •ë¥ (%)']

    df1_missing_columns = [col for col in df1_required_columns if col not in df1.columns]
    df2_missing_columns = [col for col in df2_required_columns if col not in df2.columns]

    if df1_missing_columns:
        logging.error(f"âŒ ê³µê³  ë°ì´í„°ì—ì„œ ëˆ„ë½ëœ ì»¬ëŸ¼: {df1_missing_columns}")
        raise KeyError(f"âŒ ê³µê³  ë°ì´í„°ì—ì„œ ëˆ„ë½ëœ ì»¬ëŸ¼: {df1_missing_columns}")

    if df2_missing_columns:
        logging.error(f"âŒ íˆ¬ì°° ë°ì´í„°ì—ì„œ ëˆ„ë½ëœ ì»¬ëŸ¼: {df2_missing_columns}")
        raise KeyError(f"âŒ íˆ¬ì°° ë°ì´í„°ì—ì„œ ëˆ„ë½ëœ ì»¬ëŸ¼: {df2_missing_columns}")

    # ë°ì´í„°í”„ë ˆì„ ë³µì‚¬ (ì›ë³¸ ë°ì´í„° ë³´í˜¸)
    nt_df = df1.copy()
    bd_df = df2.copy()

    # í•„ìš” ì»¬ëŸ¼ ì¶”ì¶œ
    nt_df = nt_df[df1_required_columns]
    bd_df = bd_df[df2_required_columns]

    # ê²°ì¸¡ê°’ ì²˜ë¦¬
    nt_df.dropna(subset=["ê³µê³ ë²ˆí˜¸"], inplace=True)
    nt_df.dropna(subset=["ì˜ˆê°€ë²”ìœ„"], inplace=True)

    nt_df["íˆ¬ì°°ë¥ (%)"] = nt_df["íˆ¬ì°°ë¥ (%)"].fillna(nt_df["íˆ¬ì°°ë¥ (%)"].mean(numeric_only=True))
    nt_df["ê³µê³ êµ¬ë¶„í‘œì‹œ"] = nt_df["ê³µê³ êµ¬ë¶„í‘œì‹œ"].fillna("")

    bd_df.dropna(subset=["ê³µê³ ë²ˆí˜¸"], inplace=True)

    # ê³µê³  ë°ì´í„° ì¤‘ë³µ ì œê±°
    nt_df = nt_df.drop_duplicates(subset=["ê³µê³ ë²ˆí˜¸"])

    # íˆ¬ì°° ë°ì´í„° ì‚¬ì •ë¥  ë³€í™˜ ë° ì •ë¦¬
    bd_df["ì‚¬ì •ë¥ "] = parse_sajeong_rate(bd_df["ê¸°ì´ˆëŒ€ë¹„ ì‚¬ì •ë¥ (%)"])
    bd_df.dropna(subset=["ì‚¬ì •ë¥ "], inplace=True)
    bd_df = bd_df[["ê³µê³ ë²ˆí˜¸", "ì‚¬ì •ë¥ "]]
    bd_df = group_to_list(bd_df, "ê³µê³ ë²ˆí˜¸", "ì‚¬ì •ë¥ ")

    # ë¬¸ìì—´ ì»¬ëŸ¼ ì •ë¦¬ (ê³µë°± ì œê±°)
    str_columns = ["ì˜ˆê°€ë²”ìœ„", "ë°œì£¼ì²˜(ìˆ˜ìš”ê¸°ê´€)", "ì§€ì—­ì œí•œ", "ê³µê³ êµ¬ë¶„í‘œì‹œ"]
    for col in str_columns:
        nt_df[col] = nt_df[col].astype(str).str.replace(r"\s+", "", regex=True)

    # ë°ì´í„° ë³‘í•©
    merged_data = pd.merge(nt_df, bd_df, on="ê³µê³ ë²ˆí˜¸", how="inner").reset_index(drop=True)

    logging.info(f"âœ… [restructure_data] ë°ì´í„° ì¬êµ¬ì„± ì™„ë£Œ! - ë°ì´í„° shape: {merged_data.shape}")

    return merged_data


# ============================================================
#   Seriesë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜ í•¨ìˆ˜: ë¦¬ìŠ¤íŠ¸ í™•ì¥
# ============================================================
def series_to_dataframe(series):
    """
    Pandas Seriesì˜ ê° ì›ì†Œê°€ ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš°, ì´ë¥¼ ê°œë³„ ì»¬ëŸ¼ìœ¼ë¡œ í™•ì¥í•˜ì—¬ DataFrameìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.

    Parameters:
        series (pd.Series): ë³€í™˜í•  ë°ì´í„°.

    Returns:
        pd.DataFrame: ê° ì›ì†Œê°€ ê°œë³„ ì»¬ëŸ¼ìœ¼ë¡œ í™•ì¥ëœ DataFrame.

    Raises:
        TypeError: ì…ë ¥ ë°ì´í„°ê°€ Pandas Seriesê°€ ì•„ë‹ ê²½ìš°.
    """
    # import pandas as pd
    # import numpy as np
    # import logging

    logging.info(f"âœ…[series_to_dataframe] ì‹œì‘ - ì…ë ¥ Series ê¸¸ì´: {len(series)}")

    if not isinstance(series, pd.Series):
        logging.error("âŒì…ë ¥ ë°ì´í„°ëŠ” Pandas Seriesì´ì–´ì•¼ í•©ë‹ˆë‹¤!")
        raise TypeError("âŒì…ë ¥ ë°ì´í„°ëŠ” Pandas Seriesì´ì–´ì•¼ í•©ë‹ˆë‹¤!")

    # ì»¬ëŸ¼ëª… ì„¤ì • (ê¸°ë³¸ê°’: "feature" ì‚¬ìš©)
    column_name = series.name if series.name else "feature"

    # DataFrame ë³€í™˜
    expanded_df = pd.DataFrame(series.tolist())

    # ì»¬ëŸ¼ëª… ì§€ì •
    expanded_df.columns = [f"{column_name}_{i}" for i in range(expanded_df.shape[1])]

    logging.info(f"âœ…[series_to_dataframe] ì™„ë£Œ - ê²°ê³¼ shape: {expanded_df.shape}")

    return expanded_df


# ============================================================
#   ê·¸ë£¹ë³„ ë¦¬ìŠ¤íŠ¸ ìƒì„± í•¨ìˆ˜: ì§€ì • ì»¬ëŸ¼ ê·¸ë£¹í™” í›„ ë¦¬ìŠ¤íŠ¸í™”
# ============================================================
def group_to_list(df, group_col, value_col):
    """
    ì§€ì •ëœ ê·¸ë£¹ ì»¬ëŸ¼ì„ ê¸°ì¤€ìœ¼ë¡œ ë°ì´í„°ë¥¼ ê·¸ë£¹í™”í•œ í›„, í•´ë‹¹ ê·¸ë£¹ì˜ ê°’ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ì§‘ê³„í•©ë‹ˆë‹¤.

    Parameters:
        df (pd.DataFrame): ì…ë ¥ ë°ì´í„°.
        group_col (str): ê·¸ë£¹í™”í•  ì»¬ëŸ¼ ì´ë¦„ (ì˜ˆ: 'ê³µê³ ë²ˆí˜¸').
        value_col (str): ì§‘ê³„í•  ê°’ì´ ìˆëŠ” ì»¬ëŸ¼ ì´ë¦„ (ì˜ˆ: 'ì‚¬ì •ë¥ (%)').

    Returns:
        pd.DataFrame: ê·¸ë£¹ê³¼ í•´ë‹¹ ê°’ì˜ ë¦¬ìŠ¤íŠ¸ë¥¼ í¬í•¨í•œ DataFrame.

    Raises:
        TypeError: ì…ë ¥ ë°ì´í„°ê°€ Pandas DataFrameì´ ì•„ë‹ ê²½ìš°.
        KeyError: group_col ë˜ëŠ” value_colì´ DataFrameì— ì¡´ì¬í•˜ì§€ ì•Šì„ ê²½ìš°.
    """
    # import pandas as pd
    # import logging

    logging.info(f"âœ… [group_to_list] ì‹œì‘ - ê·¸ë£¹ ì»¬ëŸ¼: {group_col}, ê°’ ì»¬ëŸ¼: {value_col}")

    # ì…ë ¥ íƒ€ì… ê²€ì¦
    if not isinstance(df, pd.DataFrame):
        logging.error("âŒ ì…ë ¥ ë°ì´í„°ëŠ” Pandas DataFrameì´ì–´ì•¼ í•©ë‹ˆë‹¤!")
        raise TypeError("âŒ ì…ë ¥ ë°ì´í„°ëŠ” Pandas DataFrameì´ì–´ì•¼ í•©ë‹ˆë‹¤!")

    # group_col, value_col ì¡´ì¬ ì—¬ë¶€ í™•ì¸
    if group_col not in df.columns or value_col not in df.columns:
        logging.error(f"âŒ ì»¬ëŸ¼ '{group_col}' ë˜ëŠ” '{value_col}'ì´(ê°€) ë°ì´í„°í”„ë ˆì„ì— ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤!")
        raise KeyError(f"âŒ ì»¬ëŸ¼ '{group_col}' ë˜ëŠ” '{value_col}'ì´(ê°€) ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")

    # ê²°ì¸¡ê°’ ì œê±°í•˜ì—¬ ê·¸ë£¹í™” ì˜¤ë¥˜ ë°©ì§€
    df_filtered = df.dropna(subset=[value_col])

    # ê·¸ë£¹í™” í›„ ë¦¬ìŠ¤íŠ¸ ë³€í™˜
    grouped_df = df_filtered.groupby(group_col)[value_col].agg(list).reset_index()

    logging.info(f"âœ… [group_to_list] ì™„ë£Œ - ê²°ê³¼ shape: {grouped_df.shape}")

    return grouped_df


### taget ë°ì´í„° ì²˜ë¦¬

# ============================================================
#   ê¸°ì´ˆëŒ€ë¹„ ì‚¬ì •ë¥  íŒŒì‹± í•¨ìˆ˜: ë¬¸ìì—´ì—ì„œ ì†Œìˆ˜ì  ìˆ«ì ì¶”ì¶œ
# ============================================================
def parse_sajeong_rate(series):
    """
    'ê¸°ì´ˆëŒ€ë¹„ ì‚¬ì •ë¥ (%)' ë¬¸ìì—´ì—ì„œ ì²« ë²ˆì§¸ ì†Œìˆ˜ í˜•íƒœì˜ ìˆ«ìë¥¼ ì¶”ì¶œí•˜ì—¬ floatìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.

    Parameters:
        series (pd.Series): ë³€í™˜í•  ë¬¸ìì—´ ë°ì´í„°.

    Returns:
        pd.Series: ì¶”ì¶œëœ ìˆ«ìë¥¼ float í˜•ì‹ìœ¼ë¡œ í¬í•¨í•˜ëŠ” Series.

    Raises:
        TypeError: ì…ë ¥ê°’ì´ Pandas Seriesê°€ ì•„ë‹ ê²½ìš°.
    """
    # import pandas as pd
    # import logging

    logging.info(f"âœ… [parse_sajeong_rate] ì‹œì‘ - Series ê¸¸ì´: {len(series)}")

    # ì…ë ¥ ë°ì´í„° ê²€ì¦
    if not isinstance(series, pd.Series):
        logging.error("âŒ ì…ë ¥ ë°ì´í„°ëŠ” Pandas Seriesì´ì–´ì•¼ í•©ë‹ˆë‹¤!")
        raise TypeError("âŒ ì…ë ¥ ë°ì´í„°ëŠ” Pandas Seriesì´ì–´ì•¼ í•©ë‹ˆë‹¤!")

    # ê²°ì¸¡ê°’ ì œê±°
    series = series.dropna()

    # ì •ê·œì‹ìœ¼ë¡œ ì²« ë²ˆì§¸ ì†Œìˆ˜ í˜•íƒœ ìˆ«ì ì¶”ì¶œ
    result_series = series.astype(str).str.extract(r'(-?\d+\.\d+)')[0]

    # ìˆ«ìë¡œ ë³€í™˜ (ë³€í™˜ ì‹¤íŒ¨ ì‹œ NaN ì²˜ë¦¬)
    result_series = pd.to_numeric(result_series, errors="coerce")

    logging.info(f"âœ… [parse_sajeong_rate] ì™„ë£Œ - ê²°ê³¼ ìƒ˜í”Œ: {result_series.head(3).tolist()}")

    return result_series


# ============================================================
#    ì˜ˆê°€ë²”ìœ„ ê°’ ë§¤í•‘ í•¨ìˆ˜: ì²« í–‰ì˜ ì˜ˆê°€ë²”ìœ„ë¡œ ë²”ìœ„ ê°’ ê²°ì •
# ============================================================
def parse_range_level(range_str):
    """
    'ì˜ˆê°€ë²”ìœ„' ê°’ì„ ì½ì–´, ë¯¸ë¦¬ ì •ì˜ëœ ë§¤í•‘ì— ë”°ë¼ ì •ìˆ˜ ê°’ì„ ë°˜í™˜í•©ë‹ˆë‹¤.

    Parameters:
        range_str (str): ì˜ˆê°€ë²”ìœ„ ë¬¸ìì—´.

    Returns:
        int: ë§¤í•‘ëœ ë²”ìœ„ ê°’ (ì˜ˆ: 3, 2 ë˜ëŠ” ê¸°ë³¸ê°’ 0).

    Raises:
        TypeError: ì…ë ¥ê°’ì´ ë¬¸ìì—´ì´ ì•„ë‹ ê²½ìš°.
    """
    # import logging

    logging.info(f"âœ… [parse_range_level] ì‹œì‘ - ì…ë ¥ê°’: {range_str}")

    # ì…ë ¥ ë°ì´í„° ê²€ì¦
    if not isinstance(range_str, str):
        logging.error("âŒ ì…ë ¥ ë°ì´í„°ëŠ” ë¬¸ìì—´ì´ì–´ì•¼ í•©ë‹ˆë‹¤!")
        raise TypeError("âŒ ì…ë ¥ ë°ì´í„°ëŠ” ë¬¸ìì—´ì´ì–´ì•¼ í•©ë‹ˆë‹¤!")

    # ë¯¸ë¦¬ ì •ì˜ëœ ë§¤í•‘ ë”•ì…”ë„ˆë¦¬: ì˜ˆê°€ë²”ìœ„ ë¬¸ìì—´ê³¼ ëŒ€ì‘ë˜ëŠ” ì •ìˆ˜ ê°’
    range_mapping = {"+3%~-3%": 3, "+2%~-2%": 2}

    # ê³µë°± ì œê±° í›„ ë§¤í•‘ ê²€ìƒ‰
    range_level = range_mapping.get(range_str.strip(), 0)

    logging.info(f"âœ… [parse_range_level] ì™„ë£Œ - ë§¤í•‘ëœ ê°’: {range_level}")

    return range_level


# ==================================================================
#   êµ¬ê°„ í•˜í•œê°’, ìƒí•œê°’ ì¶”ì¶œ í•¨ìˆ˜: ì˜ˆê°€ ë²”ìœ„ì—ì„œ ì‹œì‘ì ê³¼ ëì  ì¶”ì¶œ
# ==================================================================
def extract_bounds(range_str):
    """
    ì£¼ì–´ì§„ 'ì˜ˆê°€ë²”ìœ„' ë¬¸ìì—´ì—ì„œ ë‘ ê°œì˜ í¼ì„¼íŠ¸ ê°’ì„ ì¶”ì¶œí•˜ì—¬,
    ì´ë“¤ ì¤‘ ìµœì†Œê°’ì„ Lower, ìµœëŒ€ê°’ì„ Upperë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.

    Parameters:
        s (str): ì˜ˆê°€ë²”ìœ„ ë¬¸ìì—´ (ì˜ˆ: "+3% ~ -3%").

    Returns:
        tuple(float, float): (Lower, Upper) - ì¶”ì¶œëœ ìˆ«ì ì¤‘ ìµœì†Œê°’ê³¼ ìµœëŒ€ê°’.

    Raises:
        TypeError: ì…ë ¥ê°’ì´ ë¬¸ìì—´ì´ ì•„ë‹ ê²½ìš°.
        ValueError: ì…ë ¥ê°’ì´ ìœ íš¨í•˜ì§€ ì•Šê±°ë‚˜, ë‘ ê°œì˜ í¼ì„¼íŠ¸ ê°’ ì¶”ì¶œì— ì‹¤íŒ¨í•œ ê²½ìš°.
    """
    # import pandas as pd
    # import re
    # import logging

    logging.info(f"âœ… [extract_bounds] ì‹œì‘ - ì…ë ¥ê°’: '{range_str}'")

    # ì…ë ¥ê°’ ê²€ì¦
    if not isinstance(range_str, str):
        logging.error("âŒ ì…ë ¥ ë°ì´í„°ëŠ” ë¬¸ìì—´ì´ì–´ì•¼ í•©ë‹ˆë‹¤!")
        raise TypeError("âŒ ì…ë ¥ ë°ì´í„°ëŠ” ë¬¸ìì—´ì´ì–´ì•¼ í•©ë‹ˆë‹¤!")

    if pd.isna(range_str) or range_str.strip() == "":
        logging.error(f"âŒ ìœ íš¨í•˜ì§€ ì•Šì€ ì˜ˆê°€ë²”ìœ„ ê°’ì…ë‹ˆë‹¤: '{range_str}'")
        raise ValueError(f"âŒ ìœ íš¨í•˜ì§€ ì•Šì€ ì˜ˆê°€ë²”ìœ„ ê°’ì…ë‹ˆë‹¤: '{range_str}'")

    range_str = range_str.strip()

    # ì •ê·œì‹ìœ¼ë¡œ ìˆ«ì(í¼ì„¼íŠ¸ ê¸°í˜¸ í¬í•¨) ì¶”ì¶œ
    matches = re.findall(r"([-+]?\d+(?:\.\d+)?)%", range_str)

    if len(matches) != 2:
        logging.error(f"âŒ ì˜ˆê°€ë²”ìœ„ì—ì„œ ì‹œì‘ì ê³¼ ëì ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: '{range_str}'")
        raise ValueError(f"âŒ ì˜ˆê°€ë²”ìœ„ì—ì„œ ì‹œì‘ì ê³¼ ëì ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: '{range_str}'")

    start = float(matches[0])
    end = float(matches[1])

    lower_bound = min(start, end)
    upper_bound = max(start, end)

    logging.info(f"âœ… [extract_bounds] ì™„ë£Œ - ê²°ê³¼ (Lower, Upper): ({lower_bound}, {upper_bound})")

    return lower_bound, upper_bound


# ============================================================
#   êµ¬ê°„ ê²½ê³„ê°’ ìƒì„± í•¨ìˆ˜: ì˜ˆê°€ë²”ìœ„ì— ë”°ë¼ bins ìƒì„±
# ============================================================
def generate_bins(lower_bound, upper_bound):
    """
    ì£¼ì–´ì§„ í•˜í•œ(lower_bound)ê³¼ ìƒí•œ(upper_bound)ì„ ê¸°ì¤€ìœ¼ë¡œ ì—¬ëŸ¬ ê°œì˜ êµ¬ê°„(bin) ê²½ê³„ê°’ì„ ìƒì„±í•©ë‹ˆë‹¤.

    Parameters:
        lower_bound (float): êµ¬ê°„ì˜ ìµœì†Œê°’.
        upper_bound (float): êµ¬ê°„ì˜ ìµœëŒ€ê°’.

    Returns:
        dict: ë‹¤ì–‘í•œ interval ê°’(10, 20, 50, 100)ì— ëŒ€í•´ ìƒì„±ëœ bins ë”•ì…”ë„ˆë¦¬.

    Raises:
        TypeError: ì…ë ¥ê°’ì´ ìˆ«ìê°€ ì•„ë‹ ê²½ìš°.
        ValueError: ì¤‘ë³µëœ êµ¬ê°„ ê²½ê³„ê°’ì´ ë°œìƒí•  ê²½ìš°.
    """
    # import numpy as np
    # import logging
    from collections import Counter

    logging.info(f"âœ… [generate_bins] ì‹œì‘ - lower_bound: {lower_bound}, upper_bound: {upper_bound}")

    # ì…ë ¥ê°’ ê²€ì¦
    if not isinstance(lower_bound, (int, float)) or not isinstance(upper_bound, (int, float)):
        logging.error("âŒ ì…ë ¥ê°’ì€ ìˆ«ì(float, int)ì—¬ì•¼ í•©ë‹ˆë‹¤!")
        raise TypeError("âŒ ì…ë ¥ê°’ì€ ìˆ«ì(float, int)ì—¬ì•¼ í•©ë‹ˆë‹¤!")

    if lower_bound >= upper_bound:
        logging.error("âŒ lower_bound ê°’ì€ upper_boundë³´ë‹¤ ì‘ì•„ì•¼ í•©ë‹ˆë‹¤!")
        raise ValueError("âŒ lower_bound ê°’ì€ upper_boundë³´ë‹¤ ì‘ì•„ì•¼ í•©ë‹ˆë‹¤!")

    intervals = [10, 20, 50, 100]
    bins_dict = {}

    for interval in intervals:
        # np.linspaceë¡œ êµ¬ê°„ ê²½ê³„ê°’ ìƒì„± (êµ¬ê°„ ìˆ˜ = interval + 1)
        bins = np.linspace(lower_bound, upper_bound, num=interval + 1).tolist()

        # ì¤‘ë³µ ê²€ì‚¬ (setì„ í™œìš©í•œ ìµœì í™”)
        if len(set(bins)) < len(bins):
            duplicates = [item for item, count in Counter(bins).items() if count > 1]
            logging.error(f"âš ï¸ ì¤‘ë³µëœ êµ¬ê°„ ê²½ê³„ê°’ ë°œìƒ! ì¤‘ë³µëœ ê°’: {set(duplicates)}")
            raise ValueError(f"âš ï¸ ì¤‘ë³µëœ êµ¬ê°„ ê²½ê³„ê°’ì´ ë°œìƒí–ˆìŠµë‹ˆë‹¤! ì¤‘ë³µëœ ê°’: {set(duplicates)}")

        # ì²« ë²ˆì§¸ì™€ ë§ˆì§€ë§‰ ê²½ê³„ë¥¼ -âˆ, âˆë¡œ ì„¤ì •
        bins[0] = -np.inf
        bins[-1] = np.inf
        bins_dict[interval] = bins

        logging.info(f"âœ… [generate_bins] Interval {interval}: Bins ìƒì„± ì™„ë£Œ")

    logging.info("âœ… [generate_bins] ì™„ë£Œ - ëª¨ë“  êµ¬ê°„ ìƒì„± ì™„ë£Œ")

    return bins_dict


# ============================================================
#   CSVì—ì„œ êµ¬ê°„ ê²½ê³„ê°’ ë¶ˆëŸ¬ì˜¤ê¸° í•¨ìˆ˜
# ============================================================
def load_bins(range_level):
    """
    ì§€ì •ëœ range_level (2 ë˜ëŠ” 3)ì— í•´ë‹¹í•˜ëŠ” CSV íŒŒì¼ì—ì„œ êµ¬ê°„ ê²½ê³„ê°’ì„ ë¶ˆëŸ¬ì™€,
    ì´ë¥¼ ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.

    Parameters:
        range_level (int): ë²”ìœ„ ê°’ (í—ˆìš©ê°’: 2 ë˜ëŠ” 3).

    Returns:
        dict: {êµ¬ê°„ ê°œìˆ˜: êµ¬ê°„ ê²½ê³„ê°’ ë¦¬ìŠ¤íŠ¸} í˜•íƒœì˜ ë”•ì…”ë„ˆë¦¬.

    Raises:
        TypeError: range_levelì´ ì •ìˆ˜ê°€ ì•„ë‹ ê²½ìš°.
        ValueError: range_levelì´ 2 ë˜ëŠ” 3ì´ ì•„ë‹ ê²½ìš°.
        FileNotFoundError: í•´ë‹¹ CSV íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ëŠ” ê²½ìš°.
    """
    # import os
    # import pandas as pd
    # import numpy as np
    # import logging
    from pathlib import Path

    logging.info(f"âœ… [load_bins] ì‹œì‘ - range_level: {range_level}")

    # ì…ë ¥ê°’ ê²€ì¦
    if not isinstance(range_level, int):
        logging.error("âŒ range_levelì€ ì •ìˆ˜(int)ì—¬ì•¼ í•©ë‹ˆë‹¤!")
        raise TypeError("âŒ range_levelì€ ì •ìˆ˜(int)ì—¬ì•¼ í•©ë‹ˆë‹¤!")

    if range_level not in {2, 3}:
        logging.error("âŒ ì…ë ¥ê°’ 'range_level'ì€ 2 ë˜ëŠ” 3ì´ì–´ì•¼ í•©ë‹ˆë‹¤!")
        raise ValueError("âŒ ì…ë ¥ê°’ 'range_level'ì€ 2 ë˜ëŠ” 3ì´ì–´ì•¼ í•©ë‹ˆë‹¤!")

    intervals = [10, 20, 50, 100]
    bins_dict = {}

    for interval in intervals:
        csv_filename = f"intervals_{interval}_{range_level}.csv"
        intervals_csv_path = Path("intervals") / csv_filename

        logging.info(f"ğŸ” [load_bins] CSV íŒŒì¼ ê²½ë¡œ: {intervals_csv_path}")

        if not intervals_csv_path.exists():
            logging.error(f"âŒ êµ¬ê°„ ê²½ê³„ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {intervals_csv_path}")
            raise FileNotFoundError(f"âŒ êµ¬ê°„ ê²½ê³„ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {intervals_csv_path}")

        # CSV íŒŒì¼ ì½ê¸°
        interval_df = pd.read_csv(intervals_csv_path)

        if "ìƒí•œê°’" not in interval_df.columns:
            logging.error(f"âŒ CSV íŒŒì¼ì—ì„œ 'ìƒí•œê°’' ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {intervals_csv_path}")
            raise KeyError(f"âŒ CSV íŒŒì¼ì—ì„œ 'ìƒí•œê°’' ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

        # 'ìƒí•œê°’' ì •ë ¬ í›„ ë¦¬ìŠ¤íŠ¸ ë³€í™˜
        upper_bounds = interval_df["ìƒí•œê°’"].iloc[:-1].sort_values().tolist()
        upper_bounds = [val * 100 for val in upper_bounds]  # í¼ì„¼íŠ¸ ë³€í™˜

        # ì²« ë²ˆì§¸ì™€ ë§ˆì§€ë§‰ ê²½ê³„ë¥¼ -âˆ, âˆë¡œ ì„¤ì •
        bins = [-np.inf] + upper_bounds + [np.inf]
        bins_dict[interval] = bins

        logging.info(f"âœ… [load_bins] {interval} êµ¬ê°„: {bins}")

    logging.info("âœ… [load_bins] ì™„ë£Œ - ëª¨ë“  êµ¬ê°„ ë¡œë“œ ì™„ë£Œ")

    return bins_dict


# ============================================================
#   ë°ì´í„° êµ¬ê°„í™” ë° ë¹„ìœ¨ ê³„ì‚° í•¨ìˆ˜
# ============================================================
def data_to_target(data, bins):
    """
    ë°ì´í„°ë¥¼ êµ¬ê°„(bins)ì— ë”°ë¼ ë¶„í• í•˜ê³ , ê° êµ¬ê°„ì— ì†í•˜ëŠ” ë¹„ìœ¨ì„ ë¹ ë¥´ê²Œ ê³„ì‚°í•˜ëŠ” ìµœì í™” í•¨ìˆ˜.

    Parameters:
        data (list, np.array, pd.Series): êµ¬ê°„í™”í•  ìˆ«ì ë°ì´í„°.
        bins (list): êµ¬ê°„ ê²½ê³„ê°’ ë¦¬ìŠ¤íŠ¸ (ìµœì†Œ 2ê°œ ì´ìƒì˜ ê°’ í•„ìš”).

    Returns:
        dict: {êµ¬ê°„ ë ˆì´ë¸”: í•´ë‹¹ êµ¬ê°„ ë¹„ìœ¨} í˜•íƒœì˜ ë”•ì…”ë„ˆë¦¬.

    Raises:
        TypeError: dataê°€ ë¦¬ìŠ¤íŠ¸, NumPy ë°°ì—´, Pandas Seriesê°€ ì•„ë‹ ê²½ìš°.
        ValueError: binsê°€ ìµœì†Œ 2ê°œ ì´ìƒì˜ ê°’ì„ ê°€ì§„ ë¦¬ìŠ¤íŠ¸ê°€ ì•„ë‹ ê²½ìš°.
    """
    # import numpy as np
    # import pandas as pd
    # import logging

    logging.info("âœ… [data_to_target] ì‹œì‘")

    # ì…ë ¥ê°’ ê²€ì¦
    if not isinstance(data, (list, np.ndarray, pd.Series)):
        logging.error("âŒ ì…ë ¥ ë°ì´í„°ëŠ” ë¦¬ìŠ¤íŠ¸, NumPy ë°°ì—´ ë˜ëŠ” Pandas Seriesì—¬ì•¼ í•©ë‹ˆë‹¤!")
        raise TypeError("âŒ ì…ë ¥ ë°ì´í„°ëŠ” ë¦¬ìŠ¤íŠ¸, NumPy ë°°ì—´ ë˜ëŠ” Pandas Seriesì—¬ì•¼ í•©ë‹ˆë‹¤!")

    if not isinstance(bins, list) or len(bins) < 2:
        logging.error("âŒ ì…ë ¥ê°’ 'bins'ëŠ” ìµœì†Œ 2ê°œ ì´ìƒì˜ ê°’ì„ ê°€ì§„ ë¦¬ìŠ¤íŠ¸ì—¬ì•¼ í•©ë‹ˆë‹¤!")
        raise ValueError("âŒ ì…ë ¥ê°’ 'bins'ëŠ” ìµœì†Œ 2ê°œ ì´ìƒì˜ ê°’ì„ ê°€ì§„ ë¦¬ìŠ¤íŠ¸ì—¬ì•¼ í•©ë‹ˆë‹¤!")

    if len(data) == 0:
        logging.warning("âš ï¸ [data_to_target] ë¹ˆ ë°ì´í„° ì…ë ¥ë¨. ë¹ˆ ë”•ì…”ë„ˆë¦¬ ë°˜í™˜.")
        return {}

    # ë°ì´í„°ë¥¼ NumPy ë°°ì—´ë¡œ ë³€í™˜í•˜ì—¬ ì—°ì‚° ì†ë„ í–¥ìƒ
    data = np.asarray(data, dtype=float)

    # numpy.digitize() ì‚¬ìš©í•˜ì—¬ êµ¬ê°„í™”
    bin_indices = np.digitize(data, bins, right=False) - 1
    bin_indices = np.clip(bin_indices, 0, len(bins) - 2)  # ë²”ìœ„ë¥¼ ë²—ì–´ë‚˜ëŠ” ê°’ ë³´ì •

    # numpy.bincount() í™œìš©í•˜ì—¬ ê°œìˆ˜ ê³„ì‚°
    bin_counts = np.bincount(bin_indices, minlength=len(bins) - 1)

    # ë¹„ìœ¨ ê³„ì‚°
    total_count = len(data)
    ratios = bin_counts / total_count if total_count > 0 else bin_counts

    # êµ¬ê°„ ë ˆì´ë¸” ìƒì„±
    labels = [f"{i + 1:03}" for i in range(len(bins) - 1)]

    result = dict(zip(labels, ratios))

    logging.info(f"âœ… [data_to_target] ì™„ë£Œ - ê²°ê³¼: {result}")

    return result


# ============================================================
#   ë°ì´í„° êµ¬ê°„í™” ë° ë¹„ìœ¨ ê³„ì‚° (ì—¬ëŸ¬ bins ì‚¬ìš©)
# ============================================================
def process_row_fixed_bins(data, bins_dict):
    """
    ì—¬ëŸ¬ ê°œì˜ binsë¥¼ ì‚¬ìš©í•˜ì—¬ ë°ì´í„°ë¥¼ êµ¬ê°„í™”í•˜ê³  ë¹„ìœ¨ì„ ê³„ì‚°í•˜ëŠ” í•¨ìˆ˜.

    Parameters:
        data (list, np.array, pd.Series): êµ¬ê°„í™”ë¥¼ ì ìš©í•  ìˆ«ì ë°ì´í„°.
        bins_dict (dict): ë¯¸ë¦¬ ì •ì˜ëœ êµ¬ê°„ ê²½ê³„ê°’ ë”•ì…”ë„ˆë¦¬.

    Returns:
        dict: {êµ¬ê°„_ë ˆì´ë¸”: ë¹„ìœ¨} í˜•íƒœì˜ ë”•ì…”ë„ˆë¦¬.

    Raises:
        TypeError: dataê°€ ë¦¬ìŠ¤íŠ¸, NumPy ë°°ì—´, Pandas Seriesê°€ ì•„ë‹ ê²½ìš°.
        TypeError: bins_dictê°€ ë”•ì…”ë„ˆë¦¬ê°€ ì•„ë‹ ê²½ìš°.
    """
    # import numpy as np
    # import logging

    logging.info("âœ… [process_row_fixed_bins] ì‹œì‘")

    # ì…ë ¥ê°’ ê²€ì¦
    if not isinstance(data, (list, np.ndarray, pd.Series)):
        logging.error("âŒ ì…ë ¥ ë°ì´í„°ëŠ” ë¦¬ìŠ¤íŠ¸, NumPy ë°°ì—´ ë˜ëŠ” Pandas Seriesì—¬ì•¼ í•©ë‹ˆë‹¤!")
        raise TypeError("âŒ ì…ë ¥ ë°ì´í„°ëŠ” ë¦¬ìŠ¤íŠ¸, NumPy ë°°ì—´ ë˜ëŠ” Pandas Seriesì—¬ì•¼ í•©ë‹ˆë‹¤!")

    if not isinstance(bins_dict, dict):
        logging.error("âŒ ì…ë ¥ê°’ 'bins_dict'ëŠ” ë”•ì…”ë„ˆë¦¬ì—¬ì•¼ í•©ë‹ˆë‹¤!")
        raise TypeError("âŒ ì…ë ¥ê°’ 'bins_dict'ëŠ” ë”•ì…”ë„ˆë¦¬ì—¬ì•¼ í•©ë‹ˆë‹¤!")

    if not data:
        logging.warning("âš ï¸ [process_row_fixed_bins] ë¹ˆ ë°ì´í„° ì…ë ¥ë¨. ë¹ˆ ë”•ì…”ë„ˆë¦¬ ë°˜í™˜.")
        return {}

    if not bins_dict:
        logging.warning("âš ï¸ [process_row_fixed_bins] ë¹ˆ bins_dict ì…ë ¥ë¨. ë¹ˆ ë”•ì…”ë„ˆë¦¬ ë°˜í™˜.")
        return {}

    row_result = {}

    # data_to_target()ì„ ë°˜ë³µ í˜¸ì¶œí•˜ì§€ ì•Šê³ , dict.update() ì‚¬ìš©
    for bin_size, bins in bins_dict.items():
        bin_counts = data_to_target(data, bins)
        row_result.update({f"{bin_size:03}_{key}": value for key, value in bin_counts.items()})

    logging.info(f"âœ… [process_row_fixed_bins] ì™„ë£Œ - ê²°ê³¼: {row_result}")

    return row_result


# ============================================================
#   ë°ì´í„° êµ¬ê°„í™” ë° ëª©í‘œê°’ ê³„ì‚°
# ============================================================
def calculate_target(df):
    """
    ì…ë ¥ ë°ì´í„°í”„ë ˆì„(df)ì˜ 'ì˜ˆê°€ë²”ìœ„' ê°’ì„ ê¸°ì¤€ìœ¼ë¡œ ë°ì´í„°ë¥¼ êµ¬ê°„í™”í•˜ê³  ë¹„ìœ¨ì„ ê³„ì‚°í•˜ëŠ” í•¨ìˆ˜.

    Parameters:
        df (pd.DataFrame): ì…ë ¥ ë°ì´í„°.

    Returns:
        pd.DataFrame: êµ¬ê°„í™”ëœ ë¹„ìœ¨ ì •ë³´ë¥¼ í¬í•¨í•œ ìƒˆë¡œìš´ ë°ì´í„°í”„ë ˆì„.

    Raises:
        TypeError: dfê°€ Pandas DataFrameì´ ì•„ë‹ ê²½ìš°.
    """
    # import pandas as pd
    # import logging

    logging.info("âœ… [calculate_target] ì‹œì‘")

    # ì…ë ¥ê°’ ê²€ì¦
    if not isinstance(df, pd.DataFrame):
        logging.error("âŒ ì…ë ¥ ë°ì´í„°ëŠ” Pandas DataFrameì´ì–´ì•¼ í•©ë‹ˆë‹¤!")
        raise TypeError("âŒ ì…ë ¥ ë°ì´í„°ëŠ” Pandas DataFrameì´ì–´ì•¼ í•©ë‹ˆë‹¤!")

    # ì˜ˆê°€ë²”ìœ„ ë ˆë²¨ ë¶€ì—¬
    range_level = parse_range_level(df["ì˜ˆê°€ë²”ìœ„"].iloc[0])

    # df_bins = pd.DataFrame()  # ë¹ˆ DataFrame ìƒì„±
    rows = []  # ì„±ëŠ¥ ìµœì í™”ë¥¼ ìœ„í•œ ë¦¬ìŠ¤íŠ¸ ì‚¬ìš©

    if range_level in {2, 3}:
        logging.info(f"âœ… [calculate_target] ì˜ˆê°€ë²”ìœ„ ë ˆë²¨: {range_level}")

        bins_dict = load_bins(range_level)

        for row in df.itertuples():
            row_result = process_row_fixed_bins(getattr(row, "ì‚¬ì •ë¥ "), bins_dict)
            rows.append(row_result)

    elif range_level == 0:
        logging.info("âœ… [calculate_target] ì˜ˆê°€ë²”ìœ„ ë ˆë²¨: 0 ")

        for row in df.itertuples():
            lower_bound, upper_bound = extract_bounds(row.ì˜ˆê°€ë²”ìœ„)
            bins_dict = generate_bins(lower_bound, upper_bound)
            row_result = process_row_fixed_bins(getattr(row, "ì‚¬ì •ë¥ "), bins_dict)
            rows.append(row_result)

    # ë¦¬ìŠ¤íŠ¸ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜í•˜ì—¬ df_bins ìƒì„±
    if rows:
        df_bins = pd.DataFrame(rows)

    # ì›ë³¸ ë°ì´í„°ì™€ df_bins ë³‘í•©
    result_df = pd.concat([df.reset_index(drop=True), df_bins.reset_index(drop=True)], axis=1)

    logging.info("âœ… [calculate_target] ì™„ë£Œ - ë°ì´í„° ë³€í™˜ ì™„ë£Œ")

    return result_df


### ë°ì´í„° ë³€í™˜

# ==================================================================================================
#   ë°ì´í„° ì „ì²˜ë¦¬ í•¨ìˆ˜: ë¡œê·¸ ë³€í™˜, ì •ê·œí™”, ì›-í•« ì¸ì½”ë”©, í…ìŠ¤íŠ¸ ì„ë² ë”©, ì°¨ì› ì¶•ì†Œ, êµ¬ê°„ ê²½ìŸë¥  ê³„ì‚°
# ==================================================================================================
def process_data(df):
    """
    ë°ì´í„° ì „ì²˜ë¦¬ í•¨ìˆ˜: ë¡œê·¸ ë³€í™˜, ì •ê·œí™”, ì›-í•« ì¸ì½”ë”©, í…ìŠ¤íŠ¸ ì„ë² ë”©, ì°¨ì› ì¶•ì†Œ ìˆ˜í–‰.

    Parameters:
        df (pd.DataFrame): ì›ë³¸ ë°ì´í„°í”„ë ˆì„.

    Returns:
        pd.DataFrame: ì „ì²˜ë¦¬ëœ ë°ì´í„°í”„ë ˆì„.

    Raises:
        TypeError: dfê°€ Pandas DataFrameì´ ì•„ë‹ ê²½ìš°.
        ValueError: í•„ìš”í•œ ì»¬ëŸ¼ì´ ëˆ„ë½ëœ ê²½ìš°.
    """
    # import pandas as pd
    # import logging

    logging.info("âœ… [process_data] ì‹œì‘")

    # ì…ë ¥ê°’ ê²€ì¦
    if not isinstance(df, pd.DataFrame):
        logging.error("âŒ ì…ë ¥ ë°ì´í„°ëŠ” Pandas DataFrameì´ì–´ì•¼ í•©ë‹ˆë‹¤!")
        raise TypeError("âŒ ì…ë ¥ ë°ì´í„°ëŠ” Pandas DataFrameì´ì–´ì•¼ í•©ë‹ˆë‹¤!")

    required_columns = [
        'ê³µê³ ë²ˆí˜¸', 'ê³µê³ ì œëª©', 'ë°œì£¼ì²˜(ìˆ˜ìš”ê¸°ê´€)', 'ì§€ì—­ì œí•œ', 'ê¸°ì´ˆê¸ˆì•¡', 'ì˜ˆì •ê°€ê²©',
        'ì˜ˆê°€ë²”ìœ„', 'Aê°’', 'íˆ¬ì°°ë¥ (%)', 'ì°¸ì—¬ì—…ì²´ìˆ˜', 'ê³µê³ êµ¬ë¶„í‘œì‹œ', "ì‚¬ì •ë¥ "
    ]

    # ì»¬ëŸ¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
    missing_columns = [col for col in required_columns if col not in df.columns]
    if missing_columns:
        logging.error(f"âŒ ë‹¤ìŒ ì»¬ëŸ¼ì´ ë°ì´í„°í”„ë ˆì„ì— ì—†ìŠµë‹ˆë‹¤: {missing_columns}")
        raise ValueError(f"âŒ ë‹¤ìŒ ì»¬ëŸ¼ì´ ë°ì´í„°í”„ë ˆì„ì— ì—†ìŠµë‹ˆë‹¤: {missing_columns}")

    # ë°ì´í„°í”„ë ˆì„ ë³µì‚¬ (ì›ë³¸ ë³´í˜¸)
    copy_df = df.copy()

    # ë¡œê·¸ ë³€í™˜ ë° ì •ê·œí™” ì ìš©
    numeric_cols = ["ê¸°ì´ˆê¸ˆì•¡", "ì˜ˆì •ê°€ê²©", "íˆ¬ì°°ë¥ (%)"]
    for col in numeric_cols:
        copy_df[col] = log_transforming(copy_df[col])
        copy_df[f"norm_log_{col}"] = normalizing(copy_df[col])

    # Aê°’ ì²˜ë¦¬
    copy_df['Aê°’'] = copy_df['Aê°’'].astype(float) / (1 + copy_df['ê¸°ì´ˆê¸ˆì•¡'].astype(float))
    copy_df["norm_Aê°’/ê¸°ì´ˆê¸ˆì•¡"] = normalizing(copy_df['Aê°’'])

    # ì›-í•« ì¸ì½”ë”©ì„ ìœ„í•œ ì»¬ëŸ¼
    categorical_cols = ["ë°œì£¼ì²˜(ìˆ˜ìš”ê¸°ê´€)", "ì§€ì—­ì œí•œ", "ê³µê³ êµ¬ë¶„í‘œì‹œ"]
    encoded_dfs = [one_hot_encoding(copy_df[col]) for col in categorical_cols]

    # ì›ë³¸ ë°ì´í„°ì™€ ì›-í•« ì¸ì½”ë”© ê²°ê³¼ ë³‘í•©
    copy_df = pd.concat([copy_df] + encoded_dfs, axis=1)

    # í…ìŠ¤íŠ¸ ì„ë² ë”© ì ìš©
    logging.info("âœ… [process_data] í…ìŠ¤íŠ¸ ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì¤‘...")
    tokenizer, model, device = load_bge_model(model_name="BAAI/bge-m3")

    logging.info("âœ… [process_data] ê³µê³ ì œëª© ì„ë² ë”© ìˆ˜í–‰...")
    copy_df["embedding_ê³µê³ ì œëª©"] = embedding(copy_df["ê³µê³ ì œëª©"].fillna(""), tokenizer, model, device)

    # ì°¨ì› ì¶•ì†Œ ìˆ˜í–‰
    logging.info("âœ… [process_data] ì„ë² ë”© ë²¡í„° ì°¨ì› ì¶•ì†Œ ì¤‘...")
    expanded_df = series_to_dataframe(copy_df["embedding_ê³µê³ ì œëª©"])
    tmp_df = dimension_reducing_PCA(expanded_df, "ê³µê³ ì œëª©", 100)
    reduced_df = dimension_reducing_UMAP(tmp_df, "ê³µê³ ì œëª©", 20)

    # ë³‘í•©
    copy_df = pd.concat([copy_df.reset_index(drop=True), reduced_df.reset_index(drop=True)], axis=1)

    print(f"ğŸ” ë³‘í•© í›„ ê²°ì¸¡ì¹˜ê°€ í¬í•¨ëœ í–‰:\n{copy_df[copy_df.isna().any(axis=1)]}")

    # êµ¬ê°„ ê²½ìŸë¥  ê³„ì‚°
    logging.info("âœ… [process_data] êµ¬ê°„ ê²½ìŸë¥  ê³„ì‚° ì¤‘...")
    copy_df = calculate_target(copy_df)

    # ê¸°ì¡´ ì¹¼ëŸ¼ ì‚­ì œ
    copy_df.drop(['ê³µê³ ì œëª©', 'ë°œì£¼ì²˜(ìˆ˜ìš”ê¸°ê´€)', 'ì§€ì—­ì œí•œ', 'ê¸°ì´ˆê¸ˆì•¡', 'ì˜ˆì •ê°€ê²©',
                  'Aê°’', 'íˆ¬ì°°ë¥ (%)', 'ì°¸ì—¬ì—…ì²´ìˆ˜', 'ê³µê³ êµ¬ë¶„í‘œì‹œ', "ì‚¬ì •ë¥ ", "embedding_ê³µê³ ì œëª©"], axis=1, inplace=True)

    # ìµœì¢… ê²°ê³¼ ë°˜í™˜
    logging.info("âœ… [process_data] ì™„ë£Œ - ë°ì´í„° ì „ì²˜ë¦¬ ì™„ë£Œ")
    return copy_df.copy()


# ============================================================
# 24. ì „ì²´ ë°ì´í„°ì…‹ ë³€í™˜ í•¨ìˆ˜: ë°ì´í„° í´ë Œì§•ë¶€í„° Feature ë° Target ì²˜ë¦¬ê¹Œì§€
# ============================================================
def transform(df1, df2):
    """
    ê³µê³  ë°ì´í„°(df1)ì™€ íˆ¬ì°° ë°ì´í„°(df2)ë¥¼ ì…ë ¥ë°›ì•„, ë°ì´í„° í´ë Œì§•, Feature ì²˜ë¦¬, Target ì²˜ë¦¬
    ìˆœìœ¼ë¡œ ì‹¤í–‰í•œ í›„, ìµœì¢…ì ìœ¼ë¡œ ì„¸ ê°œì˜ ë°ì´í„°ì…‹(Dataset_3_df, Dataset_2_df, Dataset_etc_df)ì„ ìƒì„±í•˜ì—¬ ë°˜í™˜í•©ë‹ˆë‹¤.

    Parameters:
        df1 (pd.DataFrame): ê³µê³  ë°ì´í„°ë¥¼ í¬í•¨í•œ DataFrame.
        df2 (pd.DataFrame): íˆ¬ì°° ë°ì´í„°ë¥¼ í¬í•¨í•œ DataFrame.

    Returns:
        dict: {
            "DataSet_3": DataFrame,  # 'ì˜ˆê°€ë²”ìœ„'ê°€ +3%~-3%ì¸ ë°ì´í„°ì…‹
            "DataSet_2": DataFrame,  # 'ì˜ˆê°€ë²”ìœ„'ê°€ +2%~-2%ì¸ ë°ì´í„°ì…‹
            "DataSet_etc": DataFrame # 'ì˜ˆê°€ë²”ìœ„'ê°€ ê·¸ ì™¸ì¸ ë°ì´í„°ì…‹
        }

    Raises:
        TypeError: ì…ë ¥ ë°ì´í„°ê°€ Pandas DataFrameì´ ì•„ë‹ ê²½ìš°.
    """
    # import logging
    # import pandas as pd

    logging.info("âœ… [transform] ì‹œì‘")

    if not isinstance(df1, pd.DataFrame) or not isinstance(df2, pd.DataFrame):
        logging.error("âŒ ì…ë ¥ ë°ì´í„°ëŠ” Pandas DataFrameì´ì–´ì•¼ í•©ë‹ˆë‹¤!")
        raise TypeError("âŒ ì…ë ¥ ë°ì´í„°ëŠ” Pandas DataFrameì´ì–´ì•¼ í•©ë‹ˆë‹¤!")

    # ì›ë³¸ ë°ì´í„° ë³µì‚¬ ë° í´ë Œì§• ìˆ˜í–‰
    notices_df = df1.copy()
    bids_df = df2.copy()

    # ê³µê³  & íˆ¬ì°° ë°ì´í„° ì¬êµ¬ì„±
    merged_df = restructure_data(notices_df, bids_df)

    # ì˜ˆê°€ë²”ìœ„ë³„ ë¶„ë¦¬
    df_dict = separate_data(merged_df)
    df_3 = df_dict.get("range3")
    df_2 = df_dict.get("range2")
    df_etc = df_dict.get("others")

    # ë°ì´í„° ì „ì²˜ë¦¬ (process_data)
    Dataset_3_df = process_data(df_3)
    Dataset_2_df = process_data(df_2)
    Dataset_etc_df = process_data(df_etc)

    Dataset_dict = {
        "DataSet_3": Dataset_3_df,
        "DataSet_2": Dataset_2_df,
        "DataSet_etc": Dataset_etc_df
    }

    logging.info("ğŸ¯ [transform] ì™„ë£Œ - ë°ì´í„° ë³€í™˜ ì™„ë£Œ")

    return Dataset_dict


def load(dataset_dict):
    copy_dict = dataset_dict.copy()

    DataSet_3 = copy_dict.get("DataSet_3")
    DataSet_2 = copy_dict.get("DataSet_2")
    DataSet_etc = copy_dict.get("DataSet_etc")

    DataSet_3.to_csv("DataSet_3.csv", index=False)
    DataSet_2.to_csv("DataSet_2.csv", index=False)
    DataSet_etc.to_csv("DataSet_etc.csv", index=False)