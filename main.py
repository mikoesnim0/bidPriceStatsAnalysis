"""
ì „ì²´ ë¨¸ì‹ ëŸ¬ë‹ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
"""
import os
import argparse
import pandas as pd
import sys
sys.path.insert(0, os.path.abspath(os.path.dirname(__file__)))
import time
from tqdm import tqdm
from pathlib import Path
import sys
import numpy as np

# Add the project root to the Python path
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from src import config, data_processing, train, evaluate, utils

def main():
    # ëª…ë ¹í–‰ ì¸ì íŒŒì‹±
    parser = argparse.ArgumentParser(description='AutoGluon ML Pipeline')
    parser.add_argument('--data-only', action='store_true', help='ë°ì´í„° ì „ì²˜ë¦¬ë§Œ ì‹¤í–‰')
    parser.add_argument('--train-only', action='store_true', help='ëª¨ë¸ í•™ìŠµë§Œ ì‹¤í–‰')
    parser.add_argument('--evaluate-only', action='store_true', help='ëª¨ë¸ í‰ê°€ë§Œ ì‹¤í–‰')
    parser.add_argument('--num-targets', type=int, default=30, help='ì²˜ë¦¬í•  íƒ€ê²Ÿ ì»¬ëŸ¼ ìˆ˜ (ê¸°ë³¸ê°’: 30)')
    parser.add_argument('--dataset-key', type=str, default='2', help='ì‚¬ìš©í•  ë°ì´í„°ì…‹ í‚¤ (ê¸°ë³¸ê°’: 2)')
    parser.add_argument('--target-prefixes', type=str, default='010,020,050,100', help='ì²˜ë¦¬í•  íƒ€ê²Ÿ ì ‘ë‘ì‚¬, ì½¤ë§ˆë¡œ êµ¬ë¶„ (ê¸°ë³¸ê°’: 010,020,050,100)')
    parser.add_argument('--gpu', type=str, default='True', help='GPU ì‚¬ìš© ì—¬ë¶€ (True/False)')
    parser.add_argument('--models', type=str, default=None, help='ì‚¬ìš©í•  ëª¨ë¸ ëª©ë¡ (ì½¤ë§ˆë¡œ êµ¬ë¶„)')
    parser.add_argument('--preset', type=str, default='medium_quality_faster_train', 
                        help='AutoGluon í”„ë¦¬ì…‹')
    parser.add_argument('--verbose', type=int, default=1, help='ì¶œë ¥ ìƒì„¸ ìˆ˜ì¤€ (0: ê°„ëµ, 1: ê¸°ë³¸, 2: ìƒì„¸)')
    parser.add_argument('--generate-targets', action='store_true', help='íƒ€ê²Ÿ ì»¬ëŸ¼ ìë™ ìƒì„± (ì—†ì„ ê²½ìš°)')
    args = parser.parse_args()
    
    # GPU ì„¤ì • ì²˜ë¦¬
    use_gpu = args.gpu.lower() == 'true'
    
    # ë°ì´í„°ì…‹ í‚¤ ë° íƒ€ê²Ÿ ì ‘ë‘ì‚¬ ì²˜ë¦¬
    dataset_key = args.dataset_key
    target_prefixes = args.target_prefixes.split(',')
    
    # ëª¨ë¸ ëª©ë¡ ì²˜ë¦¬
    selected_models = None
    if args.models:
        selected_models = args.models.split(',')
    
    # ë¡œê¹… ì„¤ì •
    name = "AutoGluon ML Pipeline"
    start_time = time.time()
    name = f"{name} - {time.strftime('%Y%m%d_%H%M%S')}"
    logger = utils.setup_logger(name)
    logger.info("=== AutoGluon ML Pipeline Started ===")
    print("\nğŸš€ BidPrice ì˜ˆì¸¡ íŒŒì´í”„ë¼ì¸ì„ ì‹œì‘í•©ë‹ˆë‹¤...\n")
    
    try:
        # 1. ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬
        if not args.train_only and not args.evaluate_only:
            logger.info("Step 1: Loading and preprocessing data...")
            print("ğŸ“Š ë°ì´í„° ì „ì²˜ë¦¬ ë‹¨ê³„ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
            
            # MongoDBì—ì„œ ì§ì ‘ ë°ì´í„° ë¡œë“œ
            try:
                from statsModelsPredict.src.db_config.mongodb_handler import MongoDBHandler
                
                # MongoDB ì—°ê²°
                handler = MongoDBHandler(db_name='data_preprocessed')
                handler.connect()
                
                # ì»¬ë ‰ì…˜ ëª©ë¡ í™•ì¸
                collections = handler.db.list_collection_names()
                print(f"ì‚¬ìš© ê°€ëŠ¥í•œ ì»¬ë ‰ì…˜: {collections}")
                
                # ë°ì´í„°ì…‹ì— ë§ëŠ” ì»¬ë ‰ì…˜ í˜•ì‹ ì¤€ë¹„
                train_collections = [
                    f"preprocessed_dataset{dataset_key}_train",  # ê¸°ë³¸ í˜•ì‹
                    f"preprocessed_dataset_{dataset_key}_train", # ì–¸ë”ìŠ¤ì½”ì–´ í˜•ì‹ 
                    f"preprocessed_{dataset_key}_train",        # ì ‘ë‘ì‚¬ ë‹¤ë¥¸ í˜•ì‹
                    "preprocessed_train"                        # ê¸°ë³¸ ì»¬ë ‰ì…˜
                ]
                
                test_collections = [
                    f"preprocessed_dataset{dataset_key}_test",  # ê¸°ë³¸ í˜•ì‹
                    f"preprocessed_dataset_{dataset_key}_test", # ì–¸ë”ìŠ¤ì½”ì–´ í˜•ì‹
                    f"preprocessed_{dataset_key}_test",        # ì ‘ë‘ì‚¬ ë‹¤ë¥¸ í˜•ì‹
                    "preprocessed_test"                        # ê¸°ë³¸ ì»¬ë ‰ì…˜
                ]
                
                # ì‚¬ìš© ê°€ëŠ¥í•œ ì²« ë²ˆì§¸ ì»¬ë ‰ì…˜ ì„ íƒ
                train_collection = next((c for c in train_collections if c in collections), None)
                test_collection = next((c for c in test_collections if c in collections), None)
                
                if train_collection is None:
                    print("ê²½ê³ : í•™ìŠµ ë°ì´í„° ì»¬ë ‰ì…˜ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì„ì˜ ì»¬ë ‰ì…˜ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                    train_collection = collections[0] if collections else None
                
                if test_collection is None:
                    print("ê²½ê³ : í…ŒìŠ¤íŠ¸ ë°ì´í„° ì»¬ë ‰ì…˜ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. í•™ìŠµ ë°ì´í„°ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.")
                    test_collection = train_collection
                
                if train_collection is None:
                    raise ValueError("ë°ì´í„° ì»¬ë ‰ì…˜ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                
                print(f"í•™ìŠµ ë°ì´í„° ì»¬ë ‰ì…˜: {train_collection}")
                print(f"í…ŒìŠ¤íŠ¸ ë°ì´í„° ì»¬ë ‰ì…˜: {test_collection}")
                
                # í•™ìŠµ ë°ì´í„° ë¡œë“œ
                print(f"ì»¬ë ‰ì…˜ {train_collection}ì—ì„œ í•™ìŠµ ë°ì´í„° ë¡œë“œ ì¤‘...")
                train_data_list = list(handler.db[train_collection].find({}, {'_id': 0}))
                if not train_data_list:
                    raise ValueError(f"ì»¬ë ‰ì…˜ {train_collection}ì— ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                train_data = pd.DataFrame(train_data_list)
                
                # í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ
                print(f"ì»¬ë ‰ì…˜ {test_collection}ì—ì„œ í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ ì¤‘...")
                test_data_list = list(handler.db[test_collection].find({}, {'_id': 0}))
                if not test_data_list:
                    print(f"ê²½ê³ : ì»¬ë ‰ì…˜ {test_collection}ì— ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. í•™ìŠµ ë°ì´í„°ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.")
                    test_data = train_data.copy()
                else:
                    test_data = pd.DataFrame(test_data_list)
                
                # ì—°ê²° ì¢…ë£Œ
                handler.close()
                
            except Exception as e:
                logger.error(f"MongoDBì—ì„œ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
                raise
            
            # íƒ€ê²Ÿ ì»¬ëŸ¼ ì‹ë³„ ë˜ëŠ” ìƒì„±
            def identify_or_create_targets(data, target_prefixes, generate=False):
                # íƒ€ê²Ÿ ì»¬ëŸ¼ ì‹ë³„ (010_, 020_, 050_, 100_ ë¡œ ì‹œì‘í•˜ëŠ” ì»¬ëŸ¼)
                target_columns = []
                for prefix in target_prefixes:
                    prefix_cols = [col for col in data.columns if col.startswith(f"{prefix}_")]
                    target_columns.extend(prefix_cols)
                
                if not target_columns and generate:
                    print("íƒ€ê²Ÿ ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ì–´ ìë™ ìƒì„±í•©ë‹ˆë‹¤...")
                    
                    # ê°€ìƒì˜ íƒ€ê²Ÿ ì»¬ëŸ¼ ìƒì„±
                    # ê¸°ì¤€ ì»¬ëŸ¼ (ì˜ˆ: norm_log_ê¸°ì´ˆê¸ˆì•¡)ì„ ê¸°ë°˜ìœ¼ë¡œ ë³€í˜•í•˜ì—¬ íƒ€ê²Ÿ ìƒì„±
                    base_col = next((col for col in data.columns if 'norm_log' in col), None)
                    
                    if base_col is None:
                        # ì•„ë¬´ ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ì´ë‚˜ ì‚¬ìš©
                        numeric_cols = data.select_dtypes(include=['number']).columns
                        if not len(numeric_cols):
                            raise ValueError("íƒ€ê²Ÿ ìƒì„±ì— ì‚¬ìš©í•  ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.")
                        base_col = numeric_cols[0]
                    
                    # ê° íƒ€ê²Ÿ ì ‘ë‘ì‚¬ì— ëŒ€í•´ ì—¬ëŸ¬ íƒ€ê²Ÿ ìƒì„±
                    target_df = pd.DataFrame(index=data.index)
                    for prefix in target_prefixes:
                        for i in range(1, 6):  # ê° ì ‘ë‘ì‚¬ë³„ë¡œ 5ê°œ ìƒì„±
                            col_name = f"{prefix}_{i:03d}"
                            # ê¸°ë³¸ ì»¬ëŸ¼ì— ë¬´ì‘ìœ„ì„± ì¶”ê°€
                            target_df[col_name] = data[base_col] * (0.8 + np.random.rand() * 0.4) + np.random.randn(len(data)) * 0.1
                    
                    # ë‘ ë°ì´í„°í”„ë ˆì„ ê²°í•©
                    result_data = pd.concat([data, target_df], axis=1)
                    target_columns = target_df.columns.tolist()
                    
                    return result_data, target_columns
                
                return data, target_columns
            
            # ì „ì²˜ë¦¬ ë‹¨ê³„ ì§„í–‰ í‘œì‹œê¸°
            preprocessing_steps = ['ë°ì´í„° ë¡œë“œ', 'íƒ€ê²Ÿ ì²˜ë¦¬', 'ê²°ì¸¡ì¹˜ ì²˜ë¦¬', 'ë°ì´í„° ë¶„í• ', 'ë°ì´í„° ì €ì¥']
            preprocess_pbar = tqdm(preprocessing_steps, desc="ğŸ“Š ë°ì´í„° ì „ì²˜ë¦¬", position=0, leave=True)
            
            # ë°ì´í„° ë¡œë“œ ì™„ë£Œ
            preprocess_pbar.set_description("ğŸ“Š ë°ì´í„° ë¡œë“œ ì¤‘")
            preprocess_pbar.update(1)
            
            # íƒ€ê²Ÿ ì²˜ë¦¬
            preprocess_pbar.set_description("ğŸ“Š íƒ€ê²Ÿ ì²˜ë¦¬ ì¤‘")
            train_data, train_target_columns = identify_or_create_targets(
                train_data, target_prefixes, generate=args.generate_targets
            )
            test_data, test_target_columns = identify_or_create_targets(
                test_data, target_prefixes, generate=args.generate_targets
            )
            
            # ê³µí†µ íƒ€ê²Ÿ ì»¬ëŸ¼ë§Œ ì‚¬ìš©
            common_targets = sorted(list(set(train_target_columns) & set(test_target_columns)))
            if not common_targets:
                raise ValueError("í•™ìŠµ ë° í…ŒìŠ¤íŠ¸ ë°ì´í„°ì— ê³µí†µëœ íƒ€ê²Ÿ ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤!")
            
            print(f"ì‚¬ìš©í•  íƒ€ê²Ÿ ì»¬ëŸ¼: {len(common_targets)}ê°œ")
            if args.verbose > 1:
                print(f"íƒ€ê²Ÿ ì»¬ëŸ¼ ëª©ë¡: {common_targets}")
            
            # íŠ¹ì„±ê³¼ íƒ€ê²Ÿ ë¶„ë¦¬
            train_X = train_data.drop(columns=common_targets, errors='ignore')
            train_Y = train_data[common_targets]
            test_X = test_data.drop(columns=common_targets, errors='ignore')
            test_Y = test_data[common_targets]
            
            preprocess_pbar.update(1)
            
            # ê²°ì¸¡ì¹˜ ì²˜ë¦¬
            preprocess_pbar.set_description("ğŸ“Š ê²°ì¸¡ì¹˜ ì²˜ë¦¬ ì¤‘")
            
            # NaN ê°’ í™•ì¸
            nan_count_X_train = train_X.isna().sum().sum()
            nan_count_Y_train = train_Y.isna().sum().sum()
            
            if nan_count_X_train > 0:
                print(f"í•™ìŠµ íŠ¹ì„± ë°ì´í„°ì— {nan_count_X_train}ê°œì˜ NaN ê°’ì´ ìˆìŠµë‹ˆë‹¤. ì²˜ë¦¬ ì¤‘...")
                # ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ì€ ì¤‘ì•™ê°’ìœ¼ë¡œ, ë²”ì£¼í˜• ì»¬ëŸ¼ì€ ìµœë¹ˆê°’ìœ¼ë¡œ ì±„ìš°ê¸°
                for col in train_X.columns:
                    if pd.api.types.is_numeric_dtype(train_X[col]):
                        train_X[col] = train_X[col].fillna(train_X[col].median())
                    else:
                        train_X[col] = train_X[col].fillna(train_X[col].mode()[0] if not train_X[col].mode().empty else "UNKNOWN")
            
            if nan_count_Y_train > 0:
                print(f"í•™ìŠµ íƒ€ê²Ÿ ë°ì´í„°ì— {nan_count_Y_train}ê°œì˜ NaN ê°’ì´ ìˆìŠµë‹ˆë‹¤. í•´ë‹¹ í–‰ ì œê±° ì¤‘...")
                # íƒ€ê²Ÿ ê°’ì— NaNì´ ìˆëŠ” í–‰ ì œê±°
                nan_rows = train_Y.isna().any(axis=1)
                train_X = train_X[~nan_rows].reset_index(drop=True)
                train_Y = train_Y[~nan_rows].reset_index(drop=True)
            
            # í…ŒìŠ¤íŠ¸ ë°ì´í„°ë„ ë™ì¼í•˜ê²Œ ì²˜ë¦¬
            nan_count_X_test = test_X.isna().sum().sum()
            nan_count_Y_test = test_Y.isna().sum().sum()
            
            if nan_count_X_test > 0:
                print(f"í…ŒìŠ¤íŠ¸ íŠ¹ì„± ë°ì´í„°ì— {nan_count_X_test}ê°œì˜ NaN ê°’ì´ ìˆìŠµë‹ˆë‹¤. ì²˜ë¦¬ ì¤‘...")
                for col in test_X.columns:
                    if pd.api.types.is_numeric_dtype(test_X[col]):
                        test_X[col] = test_X[col].fillna(test_X[col].median())
                    else:
                        test_X[col] = test_X[col].fillna(test_X[col].mode()[0] if not test_X[col].mode().empty else "UNKNOWN")
            
            if nan_count_Y_test > 0:
                print(f"í…ŒìŠ¤íŠ¸ íƒ€ê²Ÿ ë°ì´í„°ì— {nan_count_Y_test}ê°œì˜ NaN ê°’ì´ ìˆìŠµë‹ˆë‹¤. í•´ë‹¹ í–‰ ì œê±° ì¤‘...")
                nan_rows = test_Y.isna().any(axis=1)
                test_X = test_X[~nan_rows].reset_index(drop=True)
                test_Y = test_Y[~nan_rows].reset_index(drop=True)
            
            preprocess_pbar.update(1)
            
            # ê³µí†µ íŠ¹ì„± ì»¬ëŸ¼ë§Œ ì‚¬ìš©
            common_features = sorted(list(set(train_X.columns) & set(test_X.columns)))
            if not common_features:
                raise ValueError("í•™ìŠµ ë° í…ŒìŠ¤íŠ¸ ë°ì´í„°ì— ê³µí†µëœ íŠ¹ì„± ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤!")
            
            train_X = train_X[common_features]
            test_X = test_X[common_features]
            
            # ë°ì´í„° í˜•ì‹ ì¼ì¹˜ í™•ì¸
            for col in common_features:
                if train_X[col].dtype != test_X[col].dtype:
                    # í˜•ì‹ì´ ë‹¤ë¥´ë©´ ë¬¸ìì—´ë¡œ í†µì¼
                    print(f"ì»¬ëŸ¼ {col}ì˜ ë°ì´í„° í˜•ì‹ì´ ì¼ì¹˜í•˜ì§€ ì•Šì•„ ë¬¸ìì—´ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.")
                    train_X[col] = train_X[col].astype(str)
                    test_X[col] = test_X[col].astype(str)
            
            # ë°ì´í„° ì €ì¥
            preprocess_pbar.set_description("ğŸ“Š ë°ì´í„° ì €ì¥ ì¤‘")
            train_X, test_X, train_Y, test_Y = data_processing.split_and_save_data(train_X, train_Y, test_X, test_Y)
            preprocess_pbar.update(1)
            
            print(f"\nâœ… ë°ì´í„° ì „ì²˜ë¦¬ ì™„ë£Œ! í•™ìŠµ ë°ì´í„°: {train_X.shape}, í…ŒìŠ¤íŠ¸ ë°ì´í„°: {test_X.shape}\n")
            
            if args.data_only:
                logger.info("Data processing only mode - exiting")
                print("âœ… ë°ì´í„° ì „ì²˜ë¦¬ë§Œ ìˆ˜í–‰í•˜ë„ë¡ ì„¤ì •ë˜ì–´ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                return
        else:
            logger.info("Loading preprocessed data...")
            print("ğŸ’¾ ì €ì¥ëœ ì „ì²˜ë¦¬ ë°ì´í„°ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤...")
            
            # ì „ì²˜ë¦¬ëœ ë°ì´í„° íŒŒì¼ ê²½ë¡œ í™•ì¸
            train_file = os.path.join(config.DATA_DIR, "train_data.csv")
            test_file = os.path.join(config.DATA_DIR, "test_data.csv")
            train_y_file = os.path.join(config.DATA_DIR, "train_targets.csv")
            test_y_file = os.path.join(config.DATA_DIR, "test_targets.csv")
            
            # íŒŒì¼ ì¡´ì¬ í™•ì¸
            required_files = [train_file, test_file, train_y_file, test_y_file]
            for file_path in required_files:
                if not os.path.exists(file_path):
                    raise FileNotFoundError(f"í•„ìˆ˜ ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}")
            
            # ë°ì´í„° ë¡œë“œ
            train_X = pd.read_csv(train_file)
            test_X = pd.read_csv(test_file)
            train_Y = pd.read_csv(train_y_file)
            test_Y = pd.read_csv(test_y_file)
            
            print(f"âœ… ë°ì´í„° ë¡œë“œ ì™„ë£Œ! í•™ìŠµ ë°ì´í„°: {train_X.shape}, í…ŒìŠ¤íŠ¸ ë°ì´í„°: {test_X.shape}\n")
        
        # 2. ëª¨ë¸ í•™ìŠµ
        if not args.data_only and not args.evaluate_only:
            logger.info("Step 2: Training models...")
            print("ğŸ§  ëª¨ë¸ í•™ìŠµ ë‹¨ê³„ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
            
            # íƒ€ê²Ÿ ì ‘ë‘ì‚¬ë³„ë¡œ ì²˜ë¦¬
            total_targets = 0
            model_paths = []
            
            for target_prefix in target_prefixes:
                # í•´ë‹¹ ì ‘ë‘ì‚¬ë¡œ ì‹œì‘í•˜ëŠ” íƒ€ê²Ÿ ì»¬ëŸ¼ë“¤ ì°¾ê¸°
                prefix_targets = [col for col in train_Y.columns if col.startswith(f"{target_prefix}_")]
                
                if not prefix_targets:
                    logger.warning(f"âš ï¸ ì ‘ë‘ì‚¬ '{target_prefix}_'ë¡œ ì‹œì‘í•˜ëŠ” íƒ€ê²Ÿ ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.")
                    continue
                
                # ì œí•œëœ ê°œìˆ˜ë§Œ ì²˜ë¦¬
                if args.num_targets is not None:
                    prefix_targets = prefix_targets[:args.num_targets]
                
                total_targets += len(prefix_targets)
                
                # íƒ€ê²Ÿë³„ í•™ìŠµ ì§„í–‰ í‘œì‹œê¸°
                train_pbar = tqdm(total=len(prefix_targets), 
                                  desc=f"ğŸ§  {target_prefix} ëª¨ë¸ í•™ìŠµ", 
                                  position=0, 
                                  leave=True)
                
                # ê° íƒ€ê²Ÿì— ëŒ€í•´ ëª¨ë¸ í•™ìŠµ
                for i, target_col in enumerate(prefix_targets):
                    train_pbar.set_description(f"ğŸ§  [{i+1}/{len(prefix_targets)}] {target_col} í•™ìŠµ ì¤‘")
                    
                    # GPU ì‚¬ìš© ì—¬ë¶€ í‘œì‹œ
                    if use_gpu and args.verbose > 0:
                        print(f"  ğŸ”¥ GPUë¥¼ ì‚¬ìš©í•˜ì—¬ {target_col} í•™ìŠµ ì¤‘...")
                    
                    try:
                        # ë‹¨ì¼ íƒ€ê²Ÿ í•™ìŠµ
                        model_path = train.train_single_target_model(
                            X_train=train_X,
                            Y_train=train_Y, 
                            target_col=target_col,
                            dataset_key=dataset_key,
                            use_gpu=use_gpu,
                            selected_models=selected_models,
                            preset=args.preset
                        )
                        model_paths.append(model_path)
                        
                        # í•™ìŠµ ê²°ê³¼ ê°„ëµ ì¶œë ¥
                        if args.verbose > 0:
                            print(f"  âœ… {target_col} ëª¨ë¸ í•™ìŠµ ì™„ë£Œ: {model_path}")
                    
                    except Exception as e:
                        logger.error(f"âŒ {target_col} ëª¨ë¸ í•™ìŠµ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
                        print(f"  âŒ {target_col} í•™ìŠµ ì‹¤íŒ¨: {str(e)}")
                    
                    train_pbar.update(1)
                
                train_pbar.close()
            
            print(f"\nâœ… ì´ {len(model_paths)}/{total_targets}ê°œ íƒ€ê²Ÿì— ëŒ€í•œ ëª¨ë¸ í•™ìŠµ ì™„ë£Œ!\n")
            
            if args.train_only:
                logger.info("Training only mode - exiting")
                print("âœ… ëª¨ë¸ í•™ìŠµë§Œ ìˆ˜í–‰í•˜ë„ë¡ ì„¤ì •ë˜ì–´ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                return
        
        # 3. ëª¨ë¸ í‰ê°€
        if not args.data_only and not args.train_only:
            logger.info("Step 3: Evaluating models...")
            print("ğŸ“ˆ ëª¨ë¸ í‰ê°€ ë‹¨ê³„ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
            
            # í‰ê°€í•  íƒ€ê²Ÿ ìˆ˜ ê²°ì •
            target_columns = test_Y.columns
            if args.num_targets is not None:
                target_columns = target_columns[:args.num_targets]
            
            # íƒ€ê²Ÿë³„ í‰ê°€ ì§„í–‰ í‘œì‹œê¸°
            eval_pbar = tqdm(total=len(target_columns), desc="ğŸ“ˆ ëª¨ë¸ í‰ê°€", position=0, leave=True)
            
            all_results = []
            for i, target_col in enumerate(target_columns):
                eval_pbar.set_description(f"ğŸ“ˆ [{i+1}/{len(target_columns)}] {target_col} í‰ê°€ ì¤‘")
                
                # ëª¨ë¸ ê²½ë¡œ ì„¤ì •
                model_path = os.path.join(config.MODELS_DIR, target_col)
                if not os.path.exists(model_path):
                    logger.warning(f"Model for {target_col} not found at {model_path}")
                    eval_pbar.update(1)
                    continue
                
                # í‰ê°€ ê³¼ì • í‘œì‹œê¸° (ìƒì„¸ ëª¨ë“œì—ì„œë§Œ)
                if args.verbose > 1:
                    eval_steps = ['ëª¨ë¸ ë¡œë“œ', 'ì˜ˆì¸¡ ìˆ˜í–‰', 'ì„±ëŠ¥ ê³„ì‚°', 'ì‹œê°í™” ìƒì„±', 'ê²°ê³¼ ì €ì¥']
                    eval_step_pbar = tqdm(eval_steps, desc=f"  {target_col} í‰ê°€", position=1, leave=False)
                    
                    # ê° í‰ê°€ ë‹¨ê³„ ì‹œê°í™”
                    for step in eval_steps:
                        eval_step_pbar.set_description(f"  {step} ì¤‘")
                        time.sleep(0.5)  # ì‹¤ì œë¡œëŠ” í•„ìš” ì—†ì§€ë§Œ ì§„í–‰ ìƒí™©ì„ ë³´ì—¬ì£¼ê¸° ìœ„í•œ ì§€ì—°
                        eval_step_pbar.update(1)
                    
                    eval_step_pbar.close()
                
                # ëª¨ë¸ í‰ê°€
                results = evaluate.evaluate_model(
                    model_path=model_path, 
                    test_X=test_X, 
                    test_Y=test_Y, 
                    target_col=target_col
                )
                
                # ê°„ëµí•œ ê²°ê³¼ ì¶œë ¥
                if args.verbose > 0:
                    best_model = utils.get_best_model(results, 'r2_score')
                    best_r2 = results[results['model'] == best_model]['r2_score'].values[0]
                    print(f"  âœ… {target_col} í‰ê°€ ì™„ë£Œ - ìµœê³  ëª¨ë¸: {best_model} (RÂ²: {best_r2:.4f})")
                
                all_results.append(results)
                eval_pbar.update(1)
            
            eval_pbar.close()
            
            # ê²°ê³¼ ê²°í•©
            if all_results:
                combined_results = pd.concat(all_results, ignore_index=True)
                
                # ìš”ì•½ ì €ì¥
                summary_path = os.path.join(config.RESULTS_DIR, "all_models_evaluation.csv")
                combined_results.to_csv(summary_path, index=False)
                
                # í‰ê·  ì„±ëŠ¥ ì¶œë ¥
                avg_performance = combined_results.groupby('model')[config.METRICS].mean()
                print("\nğŸ“Š ëª¨ë¸ë³„ í‰ê·  ì„±ëŠ¥:")
                print(avg_performance)
                
                print(f"\nâœ… ì´ {len(target_columns)}ê°œ íƒ€ê²Ÿì— ëŒ€í•œ ëª¨ë¸ í‰ê°€ ì™„ë£Œ!")
                print(f"ğŸ“„ ì¢…í•© ê²°ê³¼ê°€ {summary_path}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\n")
            else:
                print("\nâš ï¸ í‰ê°€í•  ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ëª¨ë¸ì„ í•™ìŠµí•´ì£¼ì„¸ìš”.\n")
        
        logger.info("=== Pipeline completed successfully ===")
        print("\nğŸ‰ ì „ì²´ íŒŒì´í”„ë¼ì¸ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
        
    except Exception as e:
        logger.error(f"Error in pipeline: {e}", exc_info=True)
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        raise

if __name__ == "__main__":
    main() 